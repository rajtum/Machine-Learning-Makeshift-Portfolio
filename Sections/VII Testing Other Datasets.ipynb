{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VII Testing other Datasets",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPiTA37hD7pEF+sMMElUl6+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajtum/Machine-Learning-Makeshift-Portfolio/blob/master/Sections/VII%20Testing%20other%20Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl3thRN84hpO",
        "colab_type": "text"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbsIImsQ4qtq",
        "colab_type": "text"
      },
      "source": [
        "### A. Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLz7j66MmRjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "\n",
        "# III. Imports\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# IV. Imports\n",
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from statistics import mean\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import time\n",
        "\n",
        "# V. Imports\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "#VI. Imports\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "#Helpful imports\n",
        "import sys\n",
        "#Allows arrays to fully print in the console without truncation\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "#Method used to save .csv files\n",
        "def login_Drive():\n",
        "  \"\"\"\n",
        "  First step to saving .csv produced is connecting to your drive. Calling this function will ask you\n",
        "  to login into an gmail account and it will ask you to give permission to Google File Stream to \n",
        "  create, delete, view, and do anything on your drive when it is mounted. Since Google File Stream is\n",
        "  a Google product and these permissions are only granted for a short while, it was determined to be\n",
        "  a safe method to export DataFrames. Make sure to the read the full permissions demanded by Google\n",
        "  File Stream before continuing.\n",
        "\n",
        "  login_Drive()\n",
        "  >>> Go to this URL in a browser: https://accounts.google.com/o/oauth2/....\n",
        "  >>>\n",
        "  >>> Enter your authorization code:\n",
        "  >>> [Input Box]\n",
        "  When successful, will return\n",
        "  >>> Mounted at drive\n",
        "  \"\"\"\n",
        "  from google.colab import drive\n",
        "  drive.mount('drive')\n",
        "def save_file(output_arg, file_name_arg, save_index = False):\n",
        "  \"\"\"\n",
        "  Adds underscores to the string file_name, which then allows the file to be copied to the drive.\n",
        "  The .csv file produced will not have indices and will be saved in the general My Drive area. Pasting\n",
        "  the following returned string without the outside quotation marks should download the file.\n",
        "\n",
        "  *** Make sure to have a drive mounted before executing function. ***\n",
        "\n",
        "  *** pandas is a dependency ***\n",
        "\n",
        "  save_file(output_v, 'Predicted Sale Price Price after Imputation Model V')\n",
        "  >>> !cp Predicted_Sale_Price_Price_after_Imputation_Model_V.csv 'drive/My Drive/'\n",
        "  \"\"\"\n",
        "  file_name = file_name_arg.replace(' ', '_') + '.csv'\n",
        "  output_arg.to_csv(file_name, index = save_index)\n",
        "  return print(\"!cp \" + file_name + \" 'drive/My Drive/'\")\n",
        "def return_index(X_arg, index_df):\n",
        "  \"\"\"\n",
        "  Takes in a dataframe without the 'Id' column as an index and makes the 'Id' column\n",
        "  the index.\n",
        "  \"\"\"\n",
        "  #Converts X_arg to a dictionary, which then can be converted to a DataFrame after the 'Id' colum is added\n",
        "  converted_df = {col:X_arg[col] for col in X_arg.columns}\n",
        "  converted_df['Id'] = index_df.index\n",
        "  X_indexed = pd.DataFrame(converted_df)\n",
        "  X_indexed.set_index('Id', inplace = True)\n",
        "  return X_indexed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TIXu3gy3bNh",
        "colab_type": "text"
      },
      "source": [
        "### B. Functions from Section III. Decision Tree Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khkjrn0_3aOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model_arg, X_arg, y_arg):\n",
        "  \"\"\"\n",
        "  Performs a train_test_split function to split the data into a training set and validation set.\n",
        "  As the names suggest, inputted model trains on the training set, and then the function returns\n",
        "  a list of the predictions for the validation y-values (target) and a list of the input validation\n",
        "  x-values (predictors).\n",
        "\n",
        "  train_model(my_model, predictors, target_variable)\n",
        "  >>> returns my_model.predict(predictors), target_variable\n",
        "  \"\"\"\n",
        "  train_X, val_X, train_y, val_y = train_test_split(X_arg, y_arg, random_state=1)\n",
        "\n",
        "  model_arg.fit(train_X, train_y)\n",
        "  return model_arg.predict(val_X), val_y\n",
        "\n",
        "def test_model(model_arg, X_arg, y_arg, X_test_arg):\n",
        "  \"\"\"\n",
        "  Same function as train_model except for the absence of a train_test_split function and no actual\n",
        "  y-value output like val_y in train_model. Used to fit an optimized ML model with the entire\n",
        "  predictors data before predicting with the test predictors dataset.\n",
        "  \"\"\"\n",
        "  model_arg.fit(X_arg, y_arg)\n",
        "  return model_arg.predict(X_test_arg)\n",
        "def mae(model_arg, X_arg, y_arg):\n",
        "  \"\"\"\n",
        "  Performs the same function as the mean_absolute_error with the addition of a\n",
        "  model argument that incoporates the train_model function to return the\n",
        "  mean absolute error (MAE). Note K-fold validation was added to later\n",
        "  functions.\n",
        "\n",
        "  mae(my_model, predictors, target_variable)\n",
        "  >>> returns mean_absolute_error(pred_y, val_y)\n",
        "  \"\"\"\n",
        "  pred_y, val_y = train_model(model_arg, X_arg, y_arg)\n",
        "  mae = mean_absolute_error(pred_y, val_y)\n",
        "  return mae\n",
        "\n",
        "def initialize_DT(max_leaf_nodes_arg):\n",
        "  \"\"\"\n",
        "  Creates a Decision Tree Regressor model with the specified max_leaf_nodes.\n",
        "\n",
        "  initialize_DT(200)\n",
        "  >>> DecisionTreeRegressor(max_leaf_nodes=200,random_state=1)\n",
        "  \"\"\"\n",
        "  DT_model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes_arg, random_state=1)\n",
        "  return DT_model\n",
        "\n",
        "def experiment_with_DT(max_leaf_nodes_range_arg, X_arg, y_arg, is_data_table = False):\n",
        "  \"\"\"\n",
        "  This model incorporates initialize_DT(), mae(), train_model() function to perform\n",
        "  one of two actions: (1) a list of MAE values (if is_data_table = False or there is\n",
        "  no specification) or (2) a dictionary with the keys as max_leaf_nodes (i.e. ints\n",
        "  like 2,25,126) and values as the MAE values. Note that max_leaf_nodes_range_arg\n",
        "  CANNOT be less than 2. The function finally returns the variable mae_results which\n",
        "  is either intialized as a dictionary or list depending on teh is_data_table Boolean.\n",
        "\n",
        "  experiment_with_DT(30, predictors, target, is_data_table = False)\n",
        "  >>> returns mae_results = [MAE @ 2 max_leaf_nodes],...,[MAE @ 30 max_leaf_nodes]\n",
        "\n",
        "  experiment_with_DT(30, predictors, target)\n",
        "  >>> returns mae_results = [MAE @ 2 max_leaf_nodes],...,[MAE @ 30 max_leaf_nodes]\n",
        "\n",
        "  experiment_with_DT(30, predictors, target, is_data_table = True)\n",
        "  >>> returns mae_results = {2:MAE @ 2 max_leaf_nodes,...,30:MAE @ 30 max_leaf_nodes}\n",
        "  \"\"\"\n",
        "  if is_data_table:\n",
        "    mae_results = {}\n",
        "  else:\n",
        "    mae_results = []\n",
        "\n",
        "  max_leaf_nodes_range = list(range(2,max_leaf_nodes_range_arg+1))\n",
        "  \n",
        "  for max_leaf_nodes in max_leaf_nodes_range:\n",
        "    DT_model = initialize_DT(max_leaf_nodes)\n",
        "    if is_data_table:\n",
        "      mae_results[max_leaf_nodes] = mae(DT_model, X_arg, y_arg)\n",
        "    else:\n",
        "      mae_results.append(mae(DT_model, X_arg, y_arg))\n",
        "  \n",
        "  return mae_results\n",
        "\n",
        "def optimize_DT(max_leaf_nodes_range_arg, X_arg, y_arg):\n",
        "  \"\"\"\n",
        "  Further extends experiment_with_DT by finding the lowest mean absolute error \n",
        "  (best for the ML model) and then returns the corresponding max_leaf_node\n",
        "  to the lowest MAE as a tuple such that (optimal max_leaf_node, lowest MAE).\n",
        "\n",
        "  optimize_DT(200, predictors, target)\n",
        "  >>> (71, 20123.142)\n",
        "  \"\"\"\n",
        "  mae_results = experiment_with_DT(max_leaf_nodes_range_arg, X_arg, y_arg)\n",
        "                                   \n",
        "  min_mae = min(mae_results)\n",
        "  optimal_leaf_node = mae_results.index(min_mae) + 2\n",
        "\n",
        "  return (optimal_leaf_node, round(min_mae, 3))\n",
        "\n",
        "\n",
        "### Abbreviated Functions\n",
        "### ---------------------\n",
        "### For information consult Section II on Taxonomy of Functions\n",
        "\n",
        "def ewDT(max_leaf_nodes_range_arg, X_arg, y_arg, is_data_table = False):\n",
        "  \"\"\"\n",
        "  Consult function experiment_with_DT for documentation.\n",
        "  \"\"\"\n",
        "  return experiment_with_DT(max_leaf_nodes_range_arg, X_arg, y_arg, is_data_table = is_data_table)\n",
        "def oDT(max_leaf_nodes_range_arg, X_arg, y_arg):\n",
        "  \"\"\"\n",
        "  Consult function optimize_DT for documentation.\n",
        "  \"\"\"\n",
        "  return optimize_DT(max_leaf_nodes_range_arg, X_arg, y_arg)\n",
        "def iDT(max_leaf_nodes_arg):\n",
        "  \"\"\"\n",
        "  Consult function optimize_DT for documentation.\n",
        "  \"\"\"\n",
        "  return initialize_DT(max_leaf_nodes_arg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psQmFbDj3pF7",
        "colab_type": "text"
      },
      "source": [
        "### C. Functions from Section IV. Random Forest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YTPt5Pi7XYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model_arg, X_arg, y_arg):\n",
        "  \"\"\"\n",
        "  Performs a train_test_split function to split the data into a training set and validation set.\n",
        "  As the names suggest, inputted model trains on the training set, and then the function returns\n",
        "  a list of the predictions for the validation y-values (target) and a list of the input validation\n",
        "  x-values (predictors).\n",
        "\n",
        "  train_model(my_model, predictors, target_variable)\n",
        "  >>> returns my_model.predict(predictors), target_variable\n",
        "  \"\"\"\n",
        "  train_X, val_X, train_y, val_y = train_test_split(X_arg, y_arg, random_state=1)\n",
        "\n",
        "  model_arg.fit(train_X, train_y)\n",
        "  return model_arg.predict(val_X), val_y\n",
        "\n",
        "def test_model(model_arg, X_arg, y_arg, X_test_arg):\n",
        "  \"\"\"\n",
        "  Same function as train_model except for the absence of a train_test_split function and no actual\n",
        "  y-value output like val_y in train_model. Used to fit an optimized ML model with the entire\n",
        "  predictors data before predicting with the test predictors dataset.\n",
        "  \"\"\"\n",
        "  model_arg.fit(X_arg, y_arg)\n",
        "  return model_arg.predict(X_test_arg)\n",
        "\n",
        "def mae(model_arg, X_arg, y_arg):\n",
        "  \"\"\"\n",
        "  Performs the same function as the mean_absolute_error with the addition of a\n",
        "  model argument that incoporates the train_model function to return the\n",
        "  mean absolute error (MAE).\n",
        "\n",
        "  mae(my_model, predictors, target_variable)\n",
        "  >>> returns mean_absolute_error(pred_y, val_y)\n",
        "  \"\"\"\n",
        "  pred_y, val_y = train_model(model_arg, X_arg, y_arg)\n",
        "  mae = mean_absolute_error(pred_y, val_y)\n",
        "  return mae\n",
        "\n",
        "def initialize_Forest(n_estimators_arg, max_depth_arg, not_mae=False):\n",
        "  \"\"\"\n",
        "  Creates a Random Forest Regressor model with the .\n",
        "\n",
        "  initialize_DT(200)\n",
        "  >>> DecisionTreeRegressor(max_leaf_nodes=200,random_state=1)\n",
        "  \"\"\"\n",
        "  if not_mae:  \n",
        "    Forest_model = RandomForestRegressor(n_estimators = n_estimators_arg,\n",
        "                                         max_depth = max_depth_arg,\n",
        "                                         random_state=1)\n",
        "  else:\n",
        "    Forest_model = RandomForestRegressor(n_estimators = n_estimators_arg,\n",
        "                                         max_depth = max_depth_arg,\n",
        "                                         criterion='mae',\n",
        "                                         random_state=1)    \n",
        "  return Forest_model\n",
        "\n",
        "def experiment_with_Forest(n_estimators_range_arg, max_depth_num_or_list_arg, X_arg, y_arg, is_data_table = False, NE_increment = 1, percent_complete = False):\n",
        "  \"\"\"\n",
        "  Enables testing of Random Forest Model based on a n_estimators (number of trees) and max_depth(maximum depth of tree).\n",
        "  \n",
        "  TO SPEED UP FUNCTION:\n",
        "  Change the increment of the n_estimators_range by defining NE_increment, if not defined, then defaults to \n",
        "  NE_increment = 1. The larger the increment, the less values in the range needed in computation.\n",
        "\n",
        "  TO TRACK PROGRESS:\n",
        "  Set percent_complete = True, if not percent_complete defaults to False. This is helpful for large ranges where you want\n",
        "  to see progres of function. Note the increased time of running this feature has not been tested.\n",
        "\n",
        "  TO INCREMENT:\n",
        "  Consult \"TO SPEED UP FUNCTION:\" section for incrementing n_estimators_range. To change the increment of the max_depth_range \n",
        "  increment, simply change the argument from range(lower_bound, upper_bound) to range(lower_bound, upper_bound, range) like such:\n",
        "  range(2,100, 5) increments by 5. For more information read up on range() documentation.\n",
        "\n",
        "  OPTIMIZATION (FINDING THE LOWEST MAE VALUES):\n",
        "  All output styles, whether processed as Data Table dictionary or simply an list of mae values, have the capability to be\n",
        "  optimized. For output styles, consider (1) and (3) consider isDataTable_optimize_Forest and for (2) and (4) consider\n",
        "  using the built-in optimization function optimize_Forest, which calls this method but simply outputs optimization\n",
        "  results as a dictionary.\n",
        "\n",
        "  OUTPUT STYLES:\n",
        "\n",
        "  Four possible output styles: (1) and (3) meant for tables and figures, (2) and (4) meant for optimization.\n",
        "\n",
        "  1) If max_depth_num_or_list_arg is an int and is_data_table is True, then function will return a dictionary with \n",
        "  max_depth_num and n_estimators stored in the key, and the mae stored as a key\n",
        "\n",
        "  experiment_with_Forest(5, 10, X, y, is_data_table = True)\n",
        "  >>> {'MD: 10, NT: 1': 29521.075342465752,\n",
        "  >>>  'MD: 10, NT: 2': 27075.25,\n",
        "  >>>  'MD: 10, NT: 3': 24907.708675799084,\n",
        "  >>>  'MD: 10, NT: 4': 24415.24212328767,\n",
        "  >>>  'MD: 10, NT: 5': 24446.186849315065}\n",
        "\n",
        "  2) If max_depth_num_or_list_arg is an int but is_data_table is False, then function will return a list with mae\n",
        "  values appended. Note that it is just a list of the values from dictionary produced in (1).\n",
        "\n",
        "  experiment_with_Forest(5, 10, X, y, is_data_table = False)\n",
        "  >>> [29521.075342465752,\n",
        "  >>>  27075.25,\n",
        "  >>>  24907.708675799084,\n",
        "  >>>  24415.24212328767,\n",
        "  >>>  24446.186849315065]\n",
        "\n",
        "  *** Please refer to function optimize_Forest output style (1) for implementation of the above output style. ***\n",
        "  \n",
        "  3) If max_depth_num_or_list_arg is a list and is_data_table is True, then function will return a dictionary\n",
        "  with the keys \"MD: \" max_depth (max_depth is an item within max_depth_range) and values in a 1 x 3 matrix\n",
        "  [max_depth,mae_res,n_estimators] (n_estimators is an item within n_estimators_range). \n",
        "\n",
        "  experiment_with_Forest(5, range(1, 4), X, y, is_data_table = True)\n",
        "  >>> {'MD: 1': [[1, 42865.78082191781, 1],\n",
        "  >>>            [1, 42506.74520547945, 2],\n",
        "  >>>            [1, 41979.210958904114, 3],\n",
        "  >>>            [1, 41977.42465753425, 4],\n",
        "  >>>            [1, 41950.30136986302, 5]],\n",
        "  >>>  'MD: 2': [[2, 37515.40821917808, 1],\n",
        "  >>>            [2, 37411.0301369863, 2],\n",
        "  >>>            [2, 36447.74246575342, 3],\n",
        "  >>>            [2, 35602.74246575342, 4],\n",
        "  >>>            [2, 35679.468493150685, 5]],\n",
        "  >>>  'MD: 3': [[3, 32610.49315068493, 1],\n",
        "  >>>            [3, 32101.60821917808, 2],\n",
        "  >>>            [3, 30171.314155251144, 3],\n",
        "  >>>            [3, 28976.747260273973, 4],\n",
        "  >>>            [3, 29039.537534246574, 5]]}\n",
        "\n",
        "  *** Please refer to the function for_3D_plot_Forest for implementation of the above output style. ***\n",
        "\n",
        "  4) If max_depth_num_or_list_arg is a list and is_data_table is False, then function will return a list with mae\n",
        "  values appended (the same as (2) output style). Note that it is just a list of the second item in the list of\n",
        "  values from dictionary produced in (3).\n",
        "\n",
        "  experiment_with_Forest(5, range(1,4), X, y, is_data_table = False)\n",
        "  >>> [42865.78082191781,\n",
        "  >>>  42506.74520547945,\n",
        "  >>>  41979.210958904114,\n",
        "  >>>  41977.42465753425,\n",
        "  >>>  41950.30136986302,\n",
        "  >>>  37515.40821917808,\n",
        "  >>>  37411.0301369863,\n",
        "  >>>  36447.74246575342,\n",
        "  >>>  35602.74246575342,\n",
        "  >>>  35679.468493150685,\n",
        "  >>>  32610.49315068493,\n",
        "  >>>  32101.60821917808,\n",
        "  >>>  30171.314155251144,\n",
        "  >>>  28976.747260273973,\n",
        "  >>>  29039.537534246574]\n",
        "\n",
        "  \"\"\"\n",
        "  #Initializes whether function will return a dictionary or list\n",
        "  if is_data_table:\n",
        "    mae_results = {}\n",
        "  else:\n",
        "    mae_results = []\n",
        "\n",
        "  #Creates a list from an the int arg n_estimators_range_arg starting at one and ending at the inputted int\n",
        "  #If NE_increment is set to another number, then in increments the n_estimators range\n",
        "  n_estimators_range = list(range(1, n_estimators_range_arg+1, NE_increment))\n",
        "\n",
        "  #Checks to see what data type max_depth_num_or_list is\n",
        "  if isinstance(max_depth_num_or_list_arg, int):\n",
        "    max_depth_num = max_depth_num_or_list_arg\n",
        "    #No need to run through max_depth numbers because there is only one\n",
        "    for n_estimators in n_estimators_range:\n",
        "      #Initializes model and calculuates MAE based on (X and y dataset,\n",
        "      #NOT outputted x's and y's of n_estimators and mae)\n",
        "      Forest_model = initialize_Forest(n_estimators, max_depth_num)\n",
        "      mae_res = mae(Forest_model, X_arg, y_arg)\n",
        "      #If true, adds a dictionary key (Max Depth) MD: max_depth_num, (Number of Trees) NT: n_estimators\" to\n",
        "      #gets the string \"MD : max_depth_num, NT: n_estimators\"\n",
        "      if is_data_table:\n",
        "        mae_results[\"MD: \" + str(max_depth_num) + \", NT: \" + str(n_estimators)] = mae_res\n",
        "      #Else, adds the mae value calculated to a list (Best option for optimizating MAE at a certain max_depth_num)\n",
        "      else:\n",
        "        mae_results.append(mae_res)\n",
        "      #Allows the progress of the function to be tracked in the console\n",
        "      if percent_complete:\n",
        "        print(str(round((n_estimators_range.index(n_estimators)+1)/len(n_estimators_range), 3)*100) +'%')\n",
        "  #Since max_depth_num_or_list_arg is now a list, we need two for loops and we will recieve 3-dimensional data\n",
        "  #3-dimensional because x = n_estimators, y = mae, z = max_depth\n",
        "  #For visualization, consider [.......]\n",
        "  else:\n",
        "    max_depth_range = max_depth_num_or_list_arg\n",
        "    if percent_complete:\n",
        "      #Finds total number of iterations needed to complete function\n",
        "      total_num_of_iterations = len(max_depth_range)*len(n_estimators_range)\n",
        "    for max_depth in max_depth_range:\n",
        "      #Now max_depth will be constant while it runs through all the n_estimators, then it will increase by 1 and iterate\n",
        "      for n_estimators in n_estimators_range:\n",
        "        #Intitializes model, calculates MAE with dataset, \n",
        "        #NOT outputted variables mentioned above (x = n_estimators...z = max_detph)\n",
        "        Forest_model = initialize_Forest(n_estimators, max_depth)\n",
        "        mae_res = mae(Forest_model, X_arg, y_arg)\n",
        "        #If true, prepares data for becoming a 3-dimensional data set\n",
        "        #Creates a dictionary with keys based on the max_depth, so\n",
        "        #Calling one key will return all the mae values over n_estimators\n",
        "        #at a certain max_depth, therefore making a 2-dimensional dataset\n",
        "        #Since there are multiple 2-dimensional datasets, data can produce\n",
        "        #a 3-dimensional plot\n",
        "        if is_data_table:\n",
        "          #Three variable matrix (x,y,z)\n",
        "          mae_results.setdefault(max_depth, []).append([max_depth,mae_res,n_estimators])\n",
        "        #Else, it returns an array (Best for mae optimization over max_depth_range and n_estimators)\n",
        "        else:\n",
        "          mae_results.append(mae_res)\n",
        "        if percent_complete:\n",
        "          #Finds the number of iterations completed by looking at indexes of n_estimators and max_depth\n",
        "          num_of_iterations_completed = (n_estimators_range.index(n_estimators)+1) + (max_depth_range.index(max_depth)*len(n_estimators_range))\n",
        "          num_of_iterations_completed_bef = (n_estimators_range.index(n_estimators)) + (max_depth_range.index(max_depth)*len(n_estimators_range))\n",
        "          #Calculates percentage of function done, total_num_of_iterations defined at the beginning of else statement\n",
        "          progress_percent = round((num_of_iterations_completed / total_num_of_iterations), 3) * 100\n",
        "          progress_percent_bef = round((num_of_iterations_completed_bef / total_num_of_iterations), 3) * 100\n",
        "          #Removes print if progress_precent hasn't changed\n",
        "          if (progress_percent != progress_percent_bef):\n",
        "            print(str(progress_percent) +'%')\n",
        "\n",
        "  return mae_results\n",
        "\n",
        "def optimize_Forest(n_estimators_range_arg, max_depth_num_or_list_arg, X_arg, y_arg, NE_increment_arg = 1):\n",
        "  \"\"\"\n",
        "  Optimizes a Random Forest Regressor based on a range of n_estimators and a range or\n",
        "  specified max_depth. Note that to change the n_estimators_range_arg increment, you\n",
        "  have to change NE_increment_arg NOT just NE_increment (note the arg at the end).\n",
        "  \n",
        "  Three possible input styles:\n",
        "\n",
        "  1) If max_depth_num_or_list_arg is a num:\n",
        "  \n",
        "  optimze_Forest(5, 10, X, y)\n",
        "  >>> {'max_depth': 10, 'Optimal MAE': 24415.24212328767, 'Optimal n_estimators': 4}\n",
        "\n",
        "  2) If max_depth_num_or_list_arg is a list:\n",
        "\n",
        "  optimze_Forest(5, range(1,4), X, y)\n",
        "  >>> {'Optimal MAE': 28976.747260273973,\n",
        "  >>>  'Optimal max_depth': 3,\n",
        "  >>>  'Optimal n_estimators': 4}\n",
        "\n",
        "  *** To validate these optimization results:\n",
        "\n",
        "  experiment_with_Forest(5, range(1,4), X, y, is_data_table = True)\n",
        "  >>> {'MD: 1': [[1, 42865.78082191781, 1],\n",
        "  >>>           [1, 42506.74520547945, 2],\n",
        "  >>>           [1, 41979.210958904114, 3],\n",
        "  >>>           [1, 41977.42465753425, 4],\n",
        "  >>>           [1, 41950.30136986302, 5]],\n",
        "  >>>  'MD: 2': [[2, 37515.40821917808, 1],\n",
        "  >>>           [2, 37411.0301369863, 2],\n",
        "  >>>           [2, 36447.74246575342, 3],\n",
        "  >>>           [2, 35602.74246575342, 4],\n",
        "  >>>           [2, 35679.468493150685, 5]],\n",
        "  >>>  'MD: 3': [[3, 32610.49315068493, 1],\n",
        "  >>>           [3, 32101.60821917808, 2],\n",
        "  >>>           [3, 30171.314155251144, 3],\n",
        "  >>>           [3, 28976.747260273973, 4],  <------------ Lowest MAE value\n",
        "  >>>           [3, 29039.537534246574, 5]]}\n",
        "\n",
        "  3) If max_depth_num_or_list_arg is a list with increments > 1 and NE_increment_arg is > 1\n",
        "     (i.e. any increment value, either for max_depth_range or n_estimators_range is > 1):\n",
        "\n",
        "  optimze_Forest(5, range(1,6,2), X, y, NE_increment_arg = 2)\n",
        "  >>> {'Optimal MAE': 25063.483013698627,\n",
        "  >>>  'Optimal max_depth': 5,\n",
        "  >>>  'Optimal n_estimators': 5}\n",
        "\n",
        "  *** To validate these optimization results:\n",
        "\n",
        "  experiment_with_Forest(5, range(1,6,2), X, y, NE_increment = 2, is_data_table=True)\n",
        "  >>> {'MD: 1': [[1, 42865.78082191781, 1],\n",
        "  >>>            [1, 41979.210958904114, 3],\n",
        "  >>>            [1, 41950.30136986302, 5]],\n",
        "  >>>  'MD: 3': [[3, 32610.49315068493, 1],\n",
        "  >>>            [3, 30171.314155251144, 3],\n",
        "  >>>            [3, 29039.537534246574, 5]],\n",
        "  >>>  'MD: 5': [[5, 29782.660273972604, 1],\n",
        "  >>>            [5, 25993.62511415525, 3],\n",
        "  >>>            [5, 25063.483013698627, 5]]}  <----------- Lowest MAE value\n",
        "  \"\"\"\n",
        "  #Checks to see if max_depth_num_or_list_arg is an int\n",
        "  #experiment_with_Forest is_data_table naturally set to False\n",
        "  if isinstance(max_depth_num_or_list_arg, int):\n",
        "    max_depth_num = max_depth_num_or_list_arg\n",
        "    #Tests Random Forest Regressor Model with given inputs and stores mae results in one-dimensional list\n",
        "    mae_results = experiment_with_Forest(n_estimators_range_arg, max_depth_num, X_arg, y_arg, NE_increment=NE_increment_arg)\n",
        "    #Finds the lowest MAE value in the list of mae_results\n",
        "    min_mae = min(mae_results)\n",
        "    #Locates the index of the lowest MAE value\n",
        "    opt_n_estimators = (mae_results.index(min_mae)+1)*NE_increment_arg\n",
        "    #Returns a dictionary wih optimized values, notes the max depth was not optimized here.\n",
        "    return {'max_depth':max_depth_num, 'Optimal n_estimators':opt_n_estimators, 'Optimal MAE':min_mae}\n",
        "  else:\n",
        "    max_depth_range = max_depth_num_or_list_arg\n",
        "    #Tests Random Forest Regressor Model with given inputs and stores mae results in one-dimensional list\n",
        "    mae_results = experiment_with_Forest(n_estimators_range_arg, max_depth_range, X_arg, y_arg, NE_increment=NE_increment_arg)\n",
        "    #Finds the loewst MAE value in the list of mae_results\n",
        "    min_mae = min(mae_results)\n",
        "    #Locates the index of the lowest MAE value\n",
        "    min_mae_index = mae_results.index(min_mae)\n",
        "    #Recreates n_estimators_range to locate the opt_n_estimators and opt_max_depth\n",
        "    n_estimators_range = list(range(1,n_estimators_range_arg+1,NE_increment_arg))\n",
        "    #Based on the min_mae_index, it finds the optimal max_depth and n_estimators\n",
        "    opt_n_estimators = n_estimators_range[min_mae_index % len(n_estimators_range)]\n",
        "    opt_max_depth = max_depth_range[min_mae_index // len(n_estimators_range)]\n",
        "    #Returns a dictionary wih optimized values\n",
        "    return {'Optimal max_depth':opt_max_depth, 'Optimal n_estimators':opt_n_estimators, 'Optimal MAE':min_mae}\n",
        "\n",
        "def for_3D_plot_Forest(experiment_with_Forest_res):\n",
        "  \"\"\"\n",
        "  Converts the dictionary returned in the (3) output style in function experiment_with_Forest to a \n",
        "  plottable 3D data via pyplot or plotly or some other structurally similar plotting modeule. Always\n",
        "  refer to documentation on where (x,y,z) plot because in pyplot 'z' acts at the height variable of\n",
        "  plot whereas plotly 'y' acts at the height variable plot.\n",
        "\n",
        "  *** Numpy (as np) dependency ***\n",
        "  max_depth_array, mae_array, n_estimators_array = for_3D_plot_Forest(experiment_with_Forest(5, range(1, 4), X, y, is_data_table = True))\n",
        "\n",
        "  print('max_depth_array')\n",
        "  print(max_depth_array)\n",
        "  print('mae_array')\n",
        "  print(mae_array)\n",
        "  print('n_estimators_array')\n",
        "  print(n_estimators_array)\n",
        "\n",
        "  >>> max_depth_array\n",
        "  >>> [[1. 2. 3.]\n",
        "  >>> [1. 2. 3.]\n",
        "  >>> [1. 2. 3.]\n",
        "  >>> [1. 2. 3.]\n",
        "  >>> [1. 2. 3.]]\n",
        "  >>> mae_array\n",
        "  >>> [[42865.78082192 37515.40821918 32610.49315068]\n",
        "  >>> [42506.74520548 37411.03013699 32101.60821918]\n",
        "  >>> [41979.2109589  36447.74246575 30171.31415525]\n",
        "  >>> [41977.42465753 35602.74246575 28976.74726027]\n",
        "  >>> [41950.30136986 35679.46849315 29039.53753425]]\n",
        "  >>> n_estimators_array\n",
        "  >>> [[1. 1. 1.]\n",
        "  >>> [2. 2. 2.]\n",
        "  >>> [3. 3. 3.]\n",
        "  >>> [4. 4. 4.]\n",
        "  >>> [5. 5. 5.]]\n",
        "  \"\"\"\n",
        "  #Separates the keys from the values, remember all the needed data is in the values\n",
        "  #Needed data = [max_depth,mae_res,n_estimators]\n",
        "  (key_values, data_unzipped) = zip(*experiment_with_Forest_res.items())\n",
        "  #Converted data_unzipped to an numpy array, where then data is sliced to max_depth\n",
        "  #mae_res,n_estimators accordingly, transposed, and then stored accordingly\n",
        "  data_array = np.array(data_unzipped)\n",
        "  max_depth_vals = data_array[0:,0:,0].T\n",
        "  mae_vals = data_array[0:,0:,1].T\n",
        "  n_estimators_vals = data_array[0:,0:,2].T\n",
        "\n",
        "  return max_depth_vals, mae_vals, n_estimators_vals\n",
        "\n",
        "def zoom_3D_Forest(max_depth_arg, mae_arg, n_estimators_arg, mae_upper_limit):\n",
        "  \"\"\"\n",
        "  Allows for the interactive_surface_Forest to focus on values below a certain mae_upper_limit to enhance\n",
        "  granularity of interactive graph. The return variables are the same as for_3D_plot_Forest.\n",
        "  \"\"\"\n",
        "  #Copies data, initializes important dictionaries\n",
        "  A_data_arg = {'Max Depth of Each Tree':max_depth_arg, 'Number of Trees':n_estimators_arg, 'MAE':mae_arg}\n",
        "  A_data = A_data_arg.copy()\n",
        "  pruned_data = {}\n",
        "  keys = {}\n",
        "  for param in A_data_arg.keys():\n",
        "    keys[param] = []\n",
        "\n",
        "  #Filters for data rows based on if the 'mae' value is lower then mae_upper_limit\n",
        "  A_data_test = A_data['MAE'] < mae_upper_limit\n",
        "  for k in range(0, len(A_data_test)):\n",
        "    #Clears shuttle_dict for next iteration\n",
        "    shuttle_dict = {} \n",
        "    #Iterates through A_data_test at index k and appends parameters to shuttle_dict\n",
        "    for i in range(0, len(A_data_test[k])):\n",
        "      if A_data_test[k,i]:\n",
        "        for param in keys.keys():\n",
        "          shuttle_dict.setdefault(param, []).append(A_data[param][k,i])\n",
        "    #Appends shuttle_values to pruned_data\n",
        "    for param in shuttle_dict.keys():\n",
        "      pruned_data.setdefault(param, []).append(shuttle_dict[param]) \n",
        "  return pruned_data['Max Depth of Each Tree'], pruned_data['MAE'], pruned_data['Number of Trees']\n",
        "\n",
        "def plot_wireframe_Forest(title, max_depth_vals_arg, n_estimators_vals_arg, mae_vals_arg):\n",
        "  \"\"\"\n",
        "  Plots a wireframe with the desired formatting based on the data inputted.\n",
        "\n",
        "  Dependencies\n",
        "  *** matplotlib as mpl ***\n",
        "  *** matplotlib.pyplot as plt ***\n",
        "  \"\"\"\n",
        "  #Sets parameters for figure quality and text font (for consistency across notebook)\n",
        "  mpl.rcParams['figure.dpi'] = 300\n",
        "  plt.rcParams[\"font.family\"] = \"serif\"\n",
        "\n",
        "  #Additional step needed to change the labels and title\n",
        "  sanserif = {'fontname':'serif'}\n",
        "\n",
        "  #Sets size, defines 3d plot, sets all labels and title with desired font size and label padding, and viewing angle\n",
        "  fig1=plt.figure(figsize=(20,15))\n",
        "  ax = plt.axes(projection='3d')\n",
        "  ax.plot_wireframe(max_depth_vals_arg, n_estimators_vals_arg, mae_vals_arg, color='black')\n",
        "  ax.set_title(title + ' Wireframe Plot | Optimization of Random Forest Regression Model', fontdict = sanserif, fontsize=16, pad=60)\n",
        "  ax.set_ylabel('Number of Trees', fontdict = sanserif, fontsize= 12, labelpad=10)\n",
        "  ax.set_zlabel('Mean Absolute Error', fontdict = sanserif, fontsize=12, labelpad=10)\n",
        "  ax.set_xlabel('Max Depth of Each Tree', fontdict = sanserif, fontsize=12, labelpad=10)\n",
        "  ax.view_init(50, 35)\n",
        "\n",
        "def pwFo(title, max_depth_vals_arg, n_estimators_vals_arg, mae_vals_arg):\n",
        "  \"\"\"\n",
        "  Consult plot_wireframe_Forest for documentation\n",
        "  \"\"\"\n",
        "  return plot_wireframe_Forest(title, max_depth_vals_arg, n_estimators_vals_arg, mae_vals_arg)\n",
        "\n",
        "def plot_surface_Forest(title, max_depth_vals_arg, n_estimators_vals_arg, mae_vals_arg):\n",
        "  \"\"\"\n",
        "  Plots a surface plot with the desired formatting based on the data inputted.\n",
        "\n",
        "  Dependencies\n",
        "  *** matplotlib as mpl ***\n",
        "  *** matplotlib.pyplot as plt ***\n",
        "  \"\"\"\n",
        "  #Sets parameters for figure quality and text font (for consistency across notebook)\n",
        "  mpl.rcParams['figure.dpi'] = 300\n",
        "  plt.rcParams[\"font.family\"] = \"serif\"\n",
        "\n",
        "  #Additional step needed to change the labels and title\n",
        "  sanserif = {'fontname':'serif'}\n",
        "\n",
        "  #Sets size, defines 3d plot\n",
        "  fig1=plt.figure(figsize=(20,15))\n",
        "  ax = plt.axes(projection='3d')\n",
        "  \n",
        "  #Outlines the data with more structure before surfac plot\n",
        "  ax.plot_wireframe(max_depth_vals_arg, n_estimators_vals_arg, mae_vals_arg, color='black')\n",
        "\n",
        "  #Sets all labels and title with desired font size and label padding, and viewing angle\n",
        "  ax.plot_surface(max_depth_vals_arg, n_estimators_vals_arg, mae_vals_arg, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
        "  ax.set_title(title + ' Surface Plot | Optimization of Random Forest Regression Model', fontdict = sanserif, fontsize=16, pad=60)\n",
        "  ax.set_ylabel('Number of Trees', fontdict = sanserif, fontsize= 12, labelpad=10)\n",
        "  ax.set_zlabel('Mean Absolute Error', fontdict = sanserif, fontsize=12, labelpad=10)\n",
        "  ax.set_xlabel('Max Depth of Each Tree', fontdict = sanserif, fontsize=12, labelpad=10)\n",
        "  ax.view_init(50, 35)\n",
        "\n",
        "def interactive_surface_Forest(title, max_depth_vals_arg, n_estimators_vals_arg, mae_vals_arg):\n",
        "  \"\"\"\n",
        "  Creates an interactive surface plot with the desired formatting based on the data inputted.\n",
        "\n",
        "  Dependencies\n",
        "  *** plotly.graph_objects as go ***\n",
        "  \"\"\"\n",
        "  #Creates surface figure based on data arguments\n",
        "  fig = go.Figure(data=[go.Surface(z = mae_vals_arg, x = max_depth_vals_arg, y = n_estimators_vals_arg)])\n",
        "  #Formats figure title, axis titles, dimesnions, and margins\n",
        "  fig.update_layout(title= title + ' Interactive Surface Plot | Optimization of Random Forest Regression Model', autosize=True,\n",
        "                    scene = dict(\n",
        "                      xaxis_title='Max Depth of Each Tree',\n",
        "                      yaxis_title='Number of Trees',\n",
        "                      zaxis_title='Mean Absolute Error'),\n",
        "                    width=1100, height=800,\n",
        "                    margin=dict(l=65, r=50, b=65, t=90))\n",
        "  fig.show()\n",
        "\n",
        "def isDataTable_optimize_Forest(DataTable, n_estimators_range_arg, max_depth_num_or_list_arg, NE_increment_arg = 1):\n",
        "  \"\"\"\n",
        "  Allows mae optimization for output styles (1) and (3) in the function experiment_with_Forest which output dictionaries.\n",
        "  Returns the optimal max depth, optimal number of trees (n_estimators), and optimal mae in a dictionary.\n",
        "\n",
        "  To optimize the mae value in your dictionary produced in experiment_with_Forest, simply copy over some key variables:\n",
        "\n",
        "  experiment_2_Forest = experiment_with_Forest(100, range(2,101), X, y, is_data_table=True)\n",
        "\n",
        "  Copy over the dictionary produced, n_estimators_range_arg (first arg) and max_depth_range_arg (second arg):\n",
        "\n",
        "  isDataTable_optimize_Forest(experiment_2_Forest, 100, range(2,101))\n",
        "  >>> {'Optimal MAE': 21705.35404109589,\n",
        "  >>>  'Optimal max_depth': 26,\n",
        "  >>>  'Optimal n_estimators': 58}\n",
        "  \"\"\"\n",
        "  #Converts dictionary to list of mae values (mae_results)\n",
        "  all_vals = np.array(list(DataTable.values()))\n",
        "  only_mae_vals = all_vals[0:,0:,1].flatten()\n",
        "  mae_results = list(only_mae_vals)\n",
        "\n",
        "  #Same optimization coding used in the function optimize_Forest\n",
        "  max_depth_range = max_depth_num_or_list_arg\n",
        "  #Tests Random Forest Regressor Model with given inputs and stores mae results in one-dimensional lis\n",
        "  #mae_results = experiment_with_Forest(n_estimators_range_arg, max_depth_range, X_arg, y_arg, NE_increment=NE_increment_arg)\n",
        "  #Finds the loewst MAE value in the list of mae_results\n",
        "  min_mae = min(mae_results)\n",
        "  #Locates the index of the lowest MAE value\n",
        "  min_mae_index = mae_results.index(min_mae)\n",
        "  #Recreates n_estimators_range to locate the opt_n_estimators and opt_max_depth\n",
        "  n_estimators_range = list(range(1,n_estimators_range_arg+1,NE_increment_arg))\n",
        "  #Based on the min_mae_index, it finds the optimal max_depth and n_estimators\n",
        "  opt_n_estimators = n_estimators_range[min_mae_index % len(n_estimators_range)]\n",
        "  opt_max_depth = max_depth_range[min_mae_index // len(n_estimators_range)]\n",
        "  #Returns a dictionary wih optimized values\n",
        "  return {'Optimal max_depth':opt_max_depth, 'Optimal n_estimators':opt_n_estimators, 'Optimal MAE':min_mae}\n",
        "\n",
        "def comparison_Grid_Search_Forest(n_estimators_arg, max_depth_arg, NE_increment = 1):\n",
        "  \"\"\"\n",
        "  Performs a grid search based on an int n_estimators_arg and range max_depth_arg.\n",
        "  Format is the same as the functions experiment_Forest and other related functions.\n",
        "\n",
        "  comparison_Grid_Search_Forest(5, range(2,6))\n",
        "  >>> {'Optimal MAE': 25181.08372830915,\n",
        "  >>>  'Optimal max_depth': 5,\n",
        "  >>>  'Optimal n_estimators': 5}\n",
        "  \"\"\"\n",
        "  #Performs grid search according to the arguments given\n",
        "  Grid_Search_Forest = GridSearchCV(estimator=RandomForestRegressor(),\n",
        "                                    param_grid = {\n",
        "                                        'n_estimators':range(1,n_estimators_arg+1, NE_increment),\n",
        "                                        'max_depth': max_depth_arg,\n",
        "                                    },\n",
        "                                    scoring='neg_mean_absolute_error'\n",
        "                                    )\n",
        "  Grid_Search_Optimized_Forest = Grid_Search_Forest.fit(X, y)\n",
        "  #Finds the optimal paramters and model\n",
        "  opt_params = Grid_Search_Optimized_Forest.best_params_\n",
        "  opt_Forest = Grid_Search_Optimized_Forest.best_estimator_\n",
        "  #Stores data in a comparable format\n",
        "  opt_data = {'Optimal MAE':mae(opt_Forest, X, y), 'Optimal max_depth':opt_params['max_depth'], 'Optimal n_estimators':opt_params['n_estimators']}\n",
        "  return opt_data\n",
        "\n",
        "def comparison_Forest(n_estimators_range_array_arg, max_depth_range_array_arg, NE_increment_targ = 1, MD_increment_targ = 1):\n",
        "  \"\"\"\n",
        "  Compares the Grid Search optimization module in sklearn with the optimization program written for two characteristics:\n",
        "\n",
        "  1) Execution Time (How long did it take for the function to complete?)\n",
        "  Here it creates a dictionary of four different keys for four different methods of optimization, shown in time_results below, \n",
        "  and appends time to run each program in seconds to a list stored in each dictionary's values. To export to a dataframe,\n",
        "  use the time_comparison_to_table function.\n",
        "\n",
        "  2) MAE Optimization (What was the prediction accuracy of the function?)\n",
        "  Creates a pandas Data Frame with the following columns:\n",
        "  * Time Test: the nth iteration of testing the execution duration of the functions\n",
        "  * Number of Trees Range: every POSSIBLE n_estimators parameter for plugged into the optimization function\n",
        "                           to define the Random Forest Regressor Model (RFRM)\n",
        "  * NE_increment: in the Number of Trees Range what n_estimators are ACTUALLY plugged into the optimization\n",
        "                  function do define the RFRM. For instance, if NE_increment = 10 in a Number of Trees Range \n",
        "                  of 10-31, then the optimization function ran three iterations with RFRMs with the following\n",
        "                  n_estimators = 10, then 20, and finally 30 because there was an increment of 10.\n",
        "  * Max Depth of Tree Range: same principle as Number of Trees Range.\n",
        "  * MD_increment: same principle as NE_increment. For instance, if MD_increment = 10 in a Max Depth of Tree\n",
        "                  Range of 10-31, then the optimization function will ran three iterations with RFRMs of the\n",
        "                  with following max_depth = 10, then 20, and finally 30 because ther was an increment of 10.\n",
        "  * Type of Function: Specifies whether a row result was produced with the sklearn GridSearchCV module or the\n",
        "                      programmed function.\n",
        "  * Optimal MAE: What was the lowest MAE value in the list of MAE values? List of MAE values come from running \n",
        "                 the function over multiple ranges due to the Number of Trees Range, Max Depth of Tree Range,\n",
        "                 NE_increment, MD_increment.\n",
        "  * Optimal max_depth: What max_depth parameter produced the lowest MAE?\n",
        "  * Optimal n_estimators: What n_estimators parameter produced the lowest MAE?\n",
        "\n",
        "  *** GridSearchCV, pandas is a dependencey ***\n",
        "\n",
        "  time_test, opt_test = time_comparison_Forest(range(10,21,10), range(10,21,10), NE_increment_targ=2, MD_increment_targ=2)\n",
        "  \n",
        "  (1) Execution Time\n",
        "  print(time_test)\n",
        "  >>> {'Created Data Table Optimization Times': [2.5442581176757812,\n",
        "  >>>  24.136849641799927],\n",
        "  >>>  'Grid Search Optimization Times': [2.2522621154785156, 19.751171588897705],\n",
        "  >>>  'Preloaded Data Table Optimization Times': [5.3882598876953125e-05,\n",
        "  >>>  0.00010013580322265625],\n",
        "  >>>  'Programmed Optimization Times': [2.548872470855713, 24.11039972305298]}\n",
        "\n",
        "  (2) MAE Optimization\n",
        "  print(opt_test)\n",
        "  * will print a DataFrame\n",
        "\n",
        "  IMPORTANT INFO ABOUT ARGUMENTS:\n",
        "  * Notice that inputting a range(10,21,10) in the first arg above (1) Results changes the n_estimators range evaluated in the function.\n",
        "    It jumps from 1-10 to 1-20. If there was no range increment arg then it would returned a dictionar like this 1-10,  1-11... 1-19, 1-20.\n",
        "    The 10 that dictates increments (third arg in range function) DOES NOT CHANGE THE NE_increment, it dictates the n_estimators range. \n",
        "    You have to change the NE_increment_targ (or n_estimators increment time argument) to change all of functions' increment at which it \n",
        "    evaluates MAE values.\n",
        "  * In this time comparison function example, all the mae-calculating functions had an NE_increment of 2, meaning they calculated MAE values\n",
        "    at n=1, n=3, n=5... and so one.\n",
        "  * This same rule applies for the max_depth increment.\n",
        "  * It is important you understand the distinction between the increment given in the range function and the variable argument increments.\n",
        "\n",
        "  ABOUT DICTIONARY KEYS IN (1) Results:\n",
        "  Created Data Table Optimization Times: when you run a experiment_with_Forest with is_data_table = True, and store the dictionary, which you\n",
        "  optimize. (Contrast with preloaded, where it does NOT run experiment_with_Forest and goes straight to optimizing)\n",
        "\n",
        "  Grid Search Optimization Times: time it takes for GridSearchCV to return optimized paramaters and mae values.\n",
        "  \n",
        "  Preloaded Data Table Optimization Times: Time it takes to optimize when experiment_with_Forest is already stored in a variable. (Contrast\n",
        "  with Created Data Table Optimization Times)\n",
        "\n",
        "  Programmed Optmization Times: time it takes for the function meant for optimization (i.e. optimize_Forest) to optimize model.\n",
        "  \"\"\"\n",
        "  n_est_test = n_estimators_range_array_arg\n",
        "  m_dep_test = max_depth_range_array_arg\n",
        "  time_results = {'Grid Search Optimization': [], 'Programmed Optimization': [], 'Preloaded Data Table Optimization':[],\n",
        "                  'Created Data Table Optimization': []}\n",
        "  #Arguments must be the same length, refer to doc string for acceptable argument\n",
        "  if len(n_estimators_range_array_arg) != len(max_depth_range_array_arg):\n",
        "    print(\"ERROR: the list of first argument must be the same length as the list of second argument\")\n",
        "  #Runs and times Grid Search and Programed Function\n",
        "  else:\n",
        "    #Initializes empty lists to record all the key variables of the test\n",
        "    time_test_list, n_est_range_list, m_dep_range_list, NE_incr_list, MD_incr_list  = [], [], [], [], []\n",
        "    type_func_list, mae_res_list, n_est_opt_list, m_dep_opt_list = [],[],[],[]\n",
        "    for i in range(0, len(n_estimators_range_array_arg)):\n",
        "      #Records key variables into lists, duplicates because we will record for two functions\n",
        "      for k in range(2):\n",
        "        time_test_list.append(str(i+1))\n",
        "        n_est_range_list.append('1-' + str(n_est_test[i]))\n",
        "        m_dep_range_list.append('2-' + str(m_dep_test[i]+1))\n",
        "        NE_incr_list.append(NE_increment_targ)\n",
        "        MD_incr_list.append(MD_increment_targ)\n",
        "      #Records start and end time of function and appends difference to respective key\n",
        "      # GS - Gridsearch\n",
        "      # r1, r2, r3 save results, but only r1 and r3 are added to dictionary because r2 will equal r3\n",
        "      # r3 is added for timing consistency\n",
        "      t0_GS = time.time()\n",
        "      r1 = comparison_Grid_Search_Forest(n_est_test[i], range(2, m_dep_test[i]+1, MD_increment_targ), NE_increment = NE_increment_targ)\n",
        "      t1_GS = time.time()\n",
        "      time_results['Grid Search Optimization'].append([t1_GS - t0_GS, n_est_test[i], m_dep_test[i]])\n",
        "      # PF = Programmed Function\n",
        "      t0_PF = time.time()\n",
        "      r2 = optimize_Forest(n_est_test[i], range(2, m_dep_test[i]+1, MD_increment_targ), X, y, NE_increment_arg = NE_increment_targ)\n",
        "      t1_PF = time.time()\n",
        "      time_results['Programmed Optimization'].append([t1_PF - t0_PF, n_est_test[i], m_dep_test[i]])\n",
        "      # CDT = Created Data Table\n",
        "      # PDT = Programmed Data Table\n",
        "      # For the sake of controlling the environemnt to one line of code, the both functions args were not divided over two lines, which\n",
        "      # would have improved readibility\n",
        "      t0_CDT = time.time()\n",
        "      data_table_temp = experiment_with_Forest(n_est_test[i], range(2, m_dep_test[i]+1, MD_increment_targ), X, y, is_data_table = True, NE_increment = NE_increment_targ)\n",
        "      t0_PDT = time.time()\n",
        "      r3 = isDataTable_optimize_Forest(data_table_temp, n_est_test[i], range(2, m_dep_test[i]+1, MD_increment_targ), NE_increment_arg = NE_increment_targ)\n",
        "      t1_DT = time.time()\n",
        "      time_results['Preloaded Data Table Optimization'].append([t1_DT - t0_PDT, n_est_test[i], m_dep_test[i]])\n",
        "      time_results['Created Data Table Optimization'].append([t1_DT - t0_CDT, n_est_test[i], m_dep_test[i]])\n",
        "      #Records the type of function used and mae optimization result\n",
        "      type_func_list.append('Grid Search Optimization')\n",
        "      mae_res_list.append(r1['Optimal MAE'])\n",
        "      m_dep_opt_list.append(r1['Optimal max_depth'])\n",
        "      n_est_opt_list.append(r1['Optimal n_estimators'])\n",
        "      type_func_list.append('Programmed Optimization')\n",
        "      mae_res_list.append(r3['Optimal MAE'])\n",
        "      m_dep_opt_list.append(r3['Optimal max_depth'])\n",
        "      n_est_opt_list.append(r3['Optimal n_estimators'])\n",
        "    opt_dictionary = {'Time Test':time_test_list, 'Number of Trees Range':n_est_range_list, 'Max Depth of Tree Range':m_dep_range_list,\n",
        "                   'NE_increment':NE_incr_list, 'MD_increment':MD_incr_list, 'Type of Function': type_func_list, 'Optimal MAE':mae_res_list,\n",
        "                    'Optimal Max Depth':m_dep_opt_list,'Optimal Number of Trees':n_est_opt_list}\n",
        "    optimization_results = pd.DataFrame(opt_dictionary)\n",
        "  #optimization_results = optimization_results.reindex(list(opt_dictionary.keys()))\n",
        "  return time_results, optimization_results\n",
        "\n",
        "def time_comparison_to_table(time_comp_arg):\n",
        "  \"\"\"\n",
        "  Converts the function comparison_Forest to plottable 2D table data.\n",
        "\n",
        "  *** pandas is a dependency ***\n",
        "  \"\"\"\n",
        "  time_comp_org = {}\n",
        "\n",
        "  #Finds each execution duration and appends it to a temp array, which is then\n",
        "  #added to the specific key, which is the type of function used,\n",
        "  for element in time_comp_arg:\n",
        "    temp_array = []\n",
        "    for array in range(len(time_comp_arg[element])):\n",
        "      temp_array.append(time_comp_arg[element][array][0])\n",
        "    time_comp_org[element] = temp_array\n",
        "  n_estimators_array = []\n",
        "  max_depth_array = []\n",
        "  #Adds the n_estimators and max_depth values to the dictionary\n",
        "  for array in range(len(time_comp_arg['Created Data Table Optimization'])):\n",
        "    n_estimators_array.append(time_comp_arg['Created Data Table Optimization'][array][1])\n",
        "    max_depth_array.append(time_comp_arg['Created Data Table Optimization'][array][2])\n",
        "  time_comp_org['Number of Trees'], time_comp_org['Max Depth of Each Tree'] = n_estimators_array, max_depth_array\n",
        "  #Creates a DataFrame and then organizations the data in the desired format\n",
        "  time_comp_table = pd.DataFrame(time_comp_org)\n",
        "  columnsTitles = ['Number of Trees', 'Max Depth of Each Tree', 'Programmed Optimization', 'Grid Search Optimization', \n",
        "                   'Created Data Table Optimization', 'Preloaded Data Table Optimization']\n",
        "  time_comp_table = time_comp_table.reindex(columns=columnsTitles)\n",
        "  return time_comp_table\n",
        "def time_comparison_plot(graph_title, x_axis_arg, time_comp_table_arg, custom_x_axis_title=''):\n",
        "  \"\"\"\n",
        "  Creates a plotly line graph for the time comparison data based on the Data table produced\n",
        "  from the function time_comparison_to_table. The time will always be the y-axis data value\n",
        "  but the x-axis data is mutable via x_axis_arg in addition to the x-axis title.\n",
        "\n",
        "  * x_axis_arg is a expected to be a string which is a column title for the time_comp_table_arg\n",
        "    DataFrame\n",
        "\n",
        "  *** plotly is a dependency ***\n",
        "  \"\"\"\n",
        "  df = time_comp_table_arg\n",
        "  fig = go.Figure()\n",
        "  types_of_opt = ['Programmed Optimization', 'Grid Search Optimization', 'Created Data Table Optimization',\n",
        "                 'Preloaded Data Table Optimization']\n",
        "                 \n",
        "  for opt_type in types_of_opt:\n",
        "    fig.add_trace(go.Scatter(x=df[x_axis_arg], y=df[opt_type],\n",
        "                             mode='lines+markers',\n",
        "                             name=opt_type))\n",
        "  if len(custom_x_axis_title) == 0:\n",
        "    fig.update_layout(title= graph_title + ' | Optimization Time for Each Function Based on ' + x_axis_arg + ' Range',\n",
        "                      xaxis_title=x_axis_arg,\n",
        "                      yaxis_title='Time (sec)')\n",
        "  else:\n",
        "    fig.update_layout(title= graph_title + ' | Optimization Time for Each Function Based on ' + custom_x_axis_title + ' Range',\n",
        "                      xaxis_title=custom_x_axis_title,\n",
        "                      yaxis_title='Time (sec)')\n",
        "  fig.show()\n",
        "def opt_comparison_plot(graph_title, x_axis_arg, opt_comp_table_arg, custom_x_axis_title=''):\n",
        "  \"\"\"\n",
        "  Creates a plotly line graph for the optimization comparison between the sklearn GridSearchCV\n",
        "  function and the local function (the one programmed here). The lowest MAE value will always\n",
        "  be the y-axis data value but the x-axis dat is mutable via x_axis_arg in addition to the\n",
        "  x-axist title.\n",
        "\n",
        "  * x_axis_arg is a expected to be a string which is a column title for the opt_comp_table_arg\n",
        "    DataFrame\n",
        "  *** plotly is a dependency ***\n",
        "  \"\"\"\n",
        "  df = opt_comp_table_arg\n",
        "  fig = px.line(df, x=x_axis_arg, y=\"Optimal MAE\", color='Type of Function',\n",
        "              hover_data=['Type of Function']).for_each_trace(lambda t: t.update(name=t.name.split(\"=\")[1]))\n",
        "\n",
        "  if len(custom_x_axis_title) == 0:\n",
        "    fig.update_layout(title= graph_title + ' | Optimal MAE from Each Function Based on ' + x_axis_arg,\n",
        "                      xaxis_title=x_axis_arg,\n",
        "                      yaxis_title='Optimal MAE')\n",
        "  else:\n",
        "    fig.update_layout(title= graph_title + ' | Optimal MAE from Each Function Based on ' + custom_x_axis_title,\n",
        "                  xaxis_title=custom_x_axis_title,\n",
        "                  yaxis_title='Optimal MAE')\n",
        "  fig.show()\n",
        "\n",
        "### Abbreviated Functions\n",
        "### ---------------------\n",
        "### For information consult Section II on Taxonomy of Functions\n",
        "def ewFo(n_estimators_range_arg, max_depth_num_or_list_arg, X_arg, y_arg, is_data_table = False, NE_increment = 1, percent_complete = False):\n",
        "  \"\"\"\n",
        "  Consult with experiment_with_Forest for documentation.\n",
        "  \"\"\"\n",
        "  return experiment_with_Forest(n_estimators_range_arg, max_depth_num_or_list_arg, X_arg, y_arg, is_data_table = is_data_table, NE_increment = NE_increment, percent_complete = percent_complete)\n",
        "def oFo(n_estimators_range_arg, max_depth_num_or_list_arg, X_arg, y_arg, NE_increment_arg = 1):\n",
        "  \"\"\"\n",
        "  Consult optimize_Forest for documentation.\n",
        "  \"\"\"\n",
        "  return optimize_Forest(n_estimators_range_arg, max_depth_num_or_list_arg, X_arg, y_arg, NE_increment_arg = NE_increment_arg)\n",
        "def f3DpFo(experiment_with_Forest_res):\n",
        "  \"\"\"\n",
        "  Consult for_3D_plot_Forest for documentation.\n",
        "  \"\"\"\n",
        "  return for_3D_plot_Forest(experiment_with_Forest_res)\n",
        "def z3Fo(max_depth_arg, mae_arg, n_estimators_arg, mae_upper_limit):\n",
        "  \"\"\"\n",
        "  Consult zoom_3D_Forest for documentation.\n",
        "  \"\"\"\n",
        "  return zoom_3D_Forest(max_depth_arg, mae_arg, n_estimators_arg, mae_upper_limit)\n",
        "def psFo(title, max_depth_vals_arg, n_estimators_vals_arg, mae_vals_arg):\n",
        "  \"\"\"\n",
        "  Consult plot_surface_Forest for documentation.\n",
        "  \"\"\"\n",
        "  return plot_surface_Forest(title, max_depth_vals_arg, n_estimators_vals_arg, mae_vals_arg)\n",
        "def isFo(title, max_depth_vals_arg, n_estimators_vals_arg, mae_vals_arg):\n",
        "  \"\"\"\n",
        "  Consult interactive_surface_Forest for documentation.\n",
        "  \"\"\"\n",
        "  return interactive_surface_Forest(title, max_depth_vals_arg, n_estimators_vals_arg, mae_vals_arg)\n",
        "def i_oFo(DataTable, n_estimators_range_arg, max_depth_num_or_list_arg, NE_increment_arg = 1):\n",
        "  \"\"\"\n",
        "  Consult isDataTable_optimize_Forest for documentation.\n",
        "  \"\"\"\n",
        "  return isDataTable_optimize_Forest(DataTable, n_estimators_range_arg, max_depth_num_or_list_arg, NE_increment_arg = NE_increment_arg)\n",
        "def cGFo(n_estimators_arg, max_depth_arg, NE_increment = 1):\n",
        "  \"\"\"\n",
        "  Consult comparison_Grid_Search_Forest for documentation.\n",
        "  \"\"\"\n",
        "  return comparison_Grid_Search_Forest(n_estimators_arg, max_depth_arg, NE_increment = NE_increment)\n",
        "def cFo(n_estimators_range_array_arg, max_depth_range_array_arg, NE_increment_targ = 1, MD_increment_targ = 1):\n",
        "  \"\"\"\n",
        "  Consult comparison_Forest for documentation.\n",
        "  \"\"\"\n",
        "  return comparison_Forest(n_estimators_range_array_arg, max_depth_range_array_arg, NE_increment_targ = NE_increment_targ, MD_increment_targ = NE_increment_targ)\n",
        "def t_cto(time_comp_arg):\n",
        "  \"\"\"\n",
        "  Consult time_comparison_to_table for documentation.\n",
        "  \"\"\"\n",
        "  return time_comparison_to_table(time_comp_arg)\n",
        "def t_cpl(graph_title, x_axis_arg, time_comp_table_arg, custom_x_axis_title=''):\n",
        "  \"\"\"\n",
        "  Consult time_comparison_plot for documentation.\n",
        "  \"\"\"\n",
        "  return time_comparison_plot(graph_title, x_axis_arg, time_comp_table_arg, custom_x_axis_title=custom_x_axis_title)\n",
        "def o_cpl(graph_title, x_axis_arg, opt_comp_table_arg, custom_x_axis_title=''):\n",
        "  \"\"\"\n",
        "  Consult opt_comparison_plot for documentation.\n",
        "  \"\"\"\n",
        "  return opt_comparison_plot(graph_title, x_axis_arg, opt_comp_table_arg, custom_x_axis_title=custom_x_axis_title)\n",
        "def iFo(n_estimators_arg, max_depth_arg, not_mae=False):\n",
        "  \"\"\"\n",
        "  Consult initialize_Forest for documentation.\n",
        "  \"\"\"\n",
        "  return initialize_Forest(n_estimators_arg, max_depth_arg, not_mae=not_mae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRXPavSv49TW",
        "colab_type": "text"
      },
      "source": [
        "### D. Functions from Section V. Adding More Predictors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0W6d3405MFY",
        "colab_type": "text"
      },
      "source": [
        "#### Functions from Section A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObdwXXXj_w5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def impute(X_arg, drop_thresh = 2/3):\n",
        "  \"\"\"\n",
        "  Isolates the numerical data columns and then drops any column that does not have the\n",
        "  drop_thresh amount of data entries. At default, this is set to 2/3 meaning that if a\n",
        "  column does not have at least 2/3 of the rows filled with data, then it drops the \n",
        "  column. After imputing the remaining columns that meet the drop_thresh but are still\n",
        "  missing values, the funciton returns the DataFrame imputed_X\n",
        "  \"\"\"\n",
        "  #Filters for only numerical columns\n",
        "  X_arg_nums = X_arg.select_dtypes(exclude = ['object'])\n",
        "  #Finds columns with any missing values\n",
        "  missing_val_columns = [col for col in X_arg_nums.columns if X_arg_nums[col].isnull().any()]\n",
        "  #Calculates the amount of rows needed to have data for the column not to be dropped\n",
        "  drop_thresh_amount = drop_thresh * len(X_arg_nums.index)\n",
        "  #Drops any columns that do not have a minimum amount of data, set at least 2/3\n",
        "  #Removes those columns from list imputable_columns, which will be imputed\n",
        "  imputable_columns = missing_val_columns\n",
        "  for col in missing_val_columns:\n",
        "    if X_arg_nums[col].isnull().sum() >= drop_thresh_amount:\n",
        "      X_arg_nums.drop(col, axis = 0, inplace = True)\n",
        "      imputable_columns.remove(col)\n",
        "  #Imputes the colums with enough data\n",
        "  imputer = SimpleImputer()\n",
        "  imputed_X = pd.DataFrame(imputer.fit_transform(X_arg_nums))\n",
        "  #Returns columns to imputed DataFrame\n",
        "  imputed_X.columns = X_arg_nums.columns\n",
        "  return imputed_X\n",
        "def comparison_plot_surface_Forest(title, A_data, B_data, a_alpha = .8, b_alpha = .8,\n",
        "                                   A_label = '', B_label = '', a_label_sep = 0, b_label_sep = 0):\n",
        "  \"\"\"\n",
        "  Plots two surface plots with the desired formatting based on the data inputted. Arguments with\n",
        "  a letter_alpha control transparency of juxtaposed graphs, letter_label create interactive annotations\n",
        "  to help to distinguish between groups of data, and letter_label_sep offers limited control over\n",
        "  separating labels that materialize to close to one another.\n",
        "\n",
        "  Dependencies\n",
        "  *** matplotlib as mpl ***\n",
        "  *** matplotlib.pyplot as plt ***\n",
        "  \"\"\"\n",
        "  #Sets parameters for figure quality and text font (for consistency across notebook)\n",
        "  mpl.rcParams['figure.dpi'] = 300\n",
        "  plt.rcParams[\"font.family\"] = \"serif\"\n",
        "\n",
        "  #Additional step needed to change the labels and title\n",
        "  sanserif = {'fontname':'serif'}\n",
        "\n",
        "  #Sets size, defines 3d plot\n",
        "  fig1=plt.figure(figsize=(20,15))\n",
        "  ax = plt.axes(projection='3d')\n",
        "  \n",
        "  #Plots all the 3D data to a surface plot\n",
        "  #Sets all labels and title with desired font size and label padding, and viewing angle\n",
        "  ax.plot_surface(A_data['Max Depth'], A_data['Number of Trees'], A_data['MAE'], rstride=1, cstride=1, \n",
        "                  cmap='viridis', edgecolor='none', alpha =a_alpha, antialiased=True)\n",
        "  ax.plot_surface(B_data['Max Depth'], B_data['Number of Trees'], B_data['MAE'], rstride=1, cstride=1, \n",
        "                  cmap='viridis', edgecolor='none', alpha =b_alpha, antialiased=True)\n",
        "  ax.set_title(title + ' Surface Plot | Optimization of Random Forest Regression Model', fontdict = sanserif, fontsize=16, pad=60)\n",
        "  ax.set_ylabel('Number of Trees', fontdict = sanserif, fontsize= 12, labelpad=10)\n",
        "  ax.set_zlabel('Mean Absolute Error', fontdict = sanserif, fontsize=12, labelpad=10)\n",
        "  ax.set_xlabel('Max Depth of Each Tree', fontdict = sanserif, fontsize=12, labelpad=10)\n",
        "  ax.text(min(A_data['Max Depth'].flatten()) - min(A_data['Max Depth'].flatten())*.01, \n",
        "          max(A_data['Number of Trees'].flatten()), \n",
        "          min(A_data['MAE'].flatten()) + min(A_data['MAE'].flatten()) *.005 + a_label_sep,\n",
        "          A_label, fontsize = 12, style='italic')\n",
        "  ax.text(min(B_data['Max Depth'].flatten()) - min(B_data['Max Depth'].flatten())*.01, \n",
        "          max(B_data['Number of Trees'].flatten()), \n",
        "          min(B_data['MAE'].flatten()) + min(B_data['MAE'].flatten()) *.005 + b_label_sep,\n",
        "          B_label, fontsize = 12, style='italic')  \n",
        "  ax.view_init(50, 35)\n",
        "def comparison_interactive_surface_Forest(title, A_data, B_data, a_alpha = .8, b_alpha = .8,\n",
        "                                          A_label = '', B_label = '', a_label_sep = 0, b_label_sep = 0):\n",
        "  \"\"\"\n",
        "  Creates two interactive surface plots with the desired formatting based on the data inputted. Arguments with\n",
        "  a letter_alpha control transparency of juxtaposed graphs, letter_label create interactive annotations\n",
        "  to help to distinguish between groups of data, and letter_label_sep offers limited control over\n",
        "  separating labels that materialize to close to one another.\n",
        "\n",
        "  Dependencies\n",
        "  *** plotly.graph_objects as go ***\n",
        "  \"\"\"\n",
        "  #Creates surface figure based on data arguments\n",
        "  fig = go.Figure(data=[go.Surface(z = A_data['MAE'], x = A_data['Max Depth'], y = A_data['Number of Trees'], \n",
        "                                   opacity=a_alpha),\n",
        "                        go.Surface(z = B_data['MAE'], x = B_data['Max Depth'], y = B_data['Number of Trees'], \n",
        "                                   opacity=b_alpha, showscale=False)])\n",
        "  #Formats figure title, axis titles, dimesnions, and margins\n",
        "  fig.update_layout(title= title + ' Interactive Surface Plot | Optimization of Random Forest Regression Model', autosize=True,\n",
        "                    scene = dict(xaxis_title='Max Depth of Each Tree',\n",
        "                                 yaxis_title='Number of Trees',\n",
        "                                 zaxis_title='Mean Absolute Error',\n",
        "                                 annotations= [dict(showarrow=False,\n",
        "                                                    x=min(A_data['Max Depth'].flatten()),\n",
        "                                                    y=max(A_data['Number of Trees'].flatten()),\n",
        "                                                    z=min(A_data['MAE'].flatten()),\n",
        "                                                    text=A_label,\n",
        "                                                    xanchor=\"left\",\n",
        "                                                    xshift=min(A_data['Max Depth'].flatten())*.05,\n",
        "                                                    yshift=min(A_data['MAE'].flatten())*.001 + a_label_sep/100,\n",
        "                                                    opacity=0.7\n",
        "                                              ),\n",
        "                                              dict(showarrow=False,\n",
        "                                                   x=min(B_data['Max Depth'].flatten()),\n",
        "                                                   y=max(B_data['Number of Trees'].flatten()),\n",
        "                                                   z=min(B_data['MAE'].flatten()),\n",
        "                                                   text=B_label,\n",
        "                                                   xanchor=\"left\",\n",
        "                                                   xshift=min(B_data['Max Depth'].flatten())*.05,\n",
        "                                                   yshift=min(B_data['MAE'].flatten())*.001 + b_label_sep/100,\n",
        "                                                   opacity=0.7\n",
        "                                              )]\n",
        "                                 ),\n",
        "                    width=1100, height=800,\n",
        "                    margin=dict(l=65, r=50, b=65, t=90),\n",
        "                    )\n",
        "  fig.show()\n",
        "def for_3D_comp_Forest(max_depth_arg, mae_arg, n_estimators_arg):\n",
        "  \"\"\"\n",
        "  Formats the data returned in the function for_3D_plot_Forest into a dictionary that simplifies\n",
        "  the arguments for the comparison graph functions.\n",
        "  \"\"\"\n",
        "  A_data = {'Max Depth': max_depth_arg, 'MAE':mae_arg, 'Number of Trees':n_estimators_arg}\n",
        "  return A_data\n",
        "\n",
        "### Abbreviated Functions\n",
        "### ---------------------\n",
        "### For information consult Section II on Taxonomy of Functions\n",
        "def f3DcFo(max_depth_arg, mae_arg, n_estimators_arg):\n",
        "  \"\"\"\n",
        "  Consult for_3D_comp_Forest for documentation.\n",
        "  \"\"\"\n",
        "  return for_3D_comp_Forest(max_depth_arg, mae_arg, n_estimators_arg)\n",
        "\n",
        "def cpFo(title, A_data, B_data, a_alpha = .8, b_alpha = .8, A_label = '', B_label = '', a_label_sep = 0, b_label_sep = 0):\n",
        "  \"\"\"\n",
        "  Consult comparison_plot_surface_Forest for documentation.\n",
        "  \"\"\"\n",
        "  return comparison_plot_surface_Forest(title, A_data, B_data, a_alpha = a_alpha, b_alpha = b_alpha, A_label = A_label, \n",
        "                                        B_label = B_label, a_label_sep = a_label_sep, b_label_sep = b_label_sep)\n",
        "def ciFo(title, A_data, B_data, a_alpha = .8, b_alpha = .8, A_label = '', B_label = '', a_label_sep = 0, b_label_sep = 0):\n",
        "  \"\"\"\n",
        "  Consult comparison_plot_surface_Forest for documentation.\n",
        "  \"\"\"\n",
        "  return comparison_interactive_surface_Forest(title, A_data, B_data, a_alpha = a_alpha, b_alpha = b_alpha, A_label = A_label, \n",
        "                                        B_label = B_label, a_label_sep = a_label_sep, b_label_sep = b_label_sep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_pZE-Q25Z0y",
        "colab_type": "text"
      },
      "source": [
        "#### Functions from Section B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPTq1uLVqXKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(X_train_arg, X_test_arg, cardinality_thresh = 10):\n",
        "  \"\"\"\n",
        "  Performs label encoding on any columns with a cardinality above the cardinality_thresh and\n",
        "  one-hot encoding for the remaining columns.\n",
        "  \"\"\"\n",
        "  #Remove any columns without data values (do not worry about numbers, we have impute() to consider that dtype)\n",
        "  cols_missing_train = [col for col in X_train_arg.columns if X_train_arg[col].isnull().any()]\n",
        "  cols_missing_test = [col for col in X_test_arg.columns if X_test_arg[col].isnull().any()]\n",
        "  X_train = X_train_arg.drop(cols_missing_train, axis = 1)\n",
        "  X_train.drop(cols_missing_test, axis = 1, inplace = True)\n",
        "  X_test = X_test_arg.drop(cols_missing_train, axis = 1)\n",
        "  X_test.drop(cols_missing_test, axis = 1, inplace = True)\n",
        "  #Finding all the categorical data columns, one-hot encoding columns, and potential label encoding columns\n",
        "  category_columns = [col for col in X_train.columns if X_train[col].dtype == 'object']\n",
        "  one_hot_columns = [col for col in category_columns if X_train[col].nunique() <= cardinality_thresh]\n",
        "  potential_label_columns = list(set(category_columns) - set(one_hot_columns))\n",
        "  #Finds label columns by making sure all the unique values in the X_train are in the X_test\n",
        "  label_columns = []\n",
        "  #Runs through all the potential label columns\n",
        "  for col in potential_label_columns:\n",
        "    #Finds all the unique values in either training or testing dataset within the column\n",
        "    unique_vals_train = set(X_train[col])\n",
        "    unique_vals_test = set(X_test[col])\n",
        "    #If the the set of unique values equals each other, than label encoding will work, append\n",
        "    if unique_vals_train == unique_vals_test:\n",
        "      label_columns.append(col)\n",
        "    #If the length of the unique values in training set is larger than testing set, lets look to see if\n",
        "    #all the unique values in the testing set are also found in the training set (meaning the training set\n",
        "    #has a few extra unique values) If true, append\n",
        "    elif len(unique_vals_train) > len(unique_vals_test):\n",
        "      if unique_vals_test.intersection(unique_vals_train) == unique_vals_test:\n",
        "        label_columns.append(col)\n",
        "  #Create one-hot encoder and encode training and testing datasets\n",
        "  OH_encoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
        "  OH_X_train = pd.DataFrame(OH_encoder.fit_transform(X_train_arg[one_hot_columns]))\n",
        "  OH_X_test = pd.DataFrame(OH_encoder.transform(X_test_arg[one_hot_columns]))\n",
        "  #Return indices to one-hot encoder result\n",
        "  OH_X_train.index = X_train_arg.index\n",
        "  OH_X_test.index = X_test_arg.index\n",
        "  #Create label encoder and encode training and testing datasets\n",
        "  label_encoder = LabelEncoder()\n",
        "  for col in label_columns:\n",
        "    label_X_train = pd.DataFrame({col:label_encoder.fit_transform(X_train_arg[col])})\n",
        "    label_X_test = pd.DataFrame({col:label_encoder.transform(X_test_arg[col])})\n",
        "  label_X_train.index = X_train_arg.index\n",
        "  label_X_test.index = X_test_arg.index\n",
        "  #Now, we have done all the encoding, time to concatenate DataFrames\n",
        "  encoded_X_train = pd.concat([OH_X_train, label_X_train], axis = 1)\n",
        "  encoded_X_test = pd.concat([OH_X_test, label_X_test], axis = 1)\n",
        "  return encoded_X_train, encoded_X_test\n",
        "def multicomparison_plot_surface_Forest(title, A_data, B_data, C_data, \n",
        "                                        a_alpha = .8, b_alpha = .8, c_alpha = .8,\n",
        "                                        A_label = '', B_label = '', C_label = '',\n",
        "                                        a_label_sep = 0, b_label_sep = 0, c_label_sep = 0,\n",
        "                                        D_data = {}, d_alpha = .8, D_label = '', d_label_sep = 0,\n",
        "                                        E_data = {}, e_alpha = .8, E_label = '', e_label_sep = 0):\n",
        "  \"\"\"\n",
        "  Allows multiple surface plots (5 Max) with the desired formatting based on the data inputted.\n",
        "  Check the sister method comparison_plot_surface_Forest for comparisons with only two datsets.\n",
        "  Note that here dictionaries will be the arguments rather than the values of the dictionaries'\n",
        "  being directly entered, so data returned from a for_plot can be plotted with a single arguemnt.\n",
        "  Arguments with a letter_alpha control transparency of juxtaposed graphs, letter_label create \n",
        "  interactive annotations to help to distinguish between groups of data, and letter_label_sep \n",
        "  offers limited control over separating labels that materialize to close to one another.\n",
        "\n",
        "  Dependencies\n",
        "  *** matplotlib as mpl ***\n",
        "  *** matplotlib.pyplot as plt ***\n",
        "  \"\"\"\n",
        "  #Sets parameters for figure quality and text font (for consistency across notebook)\n",
        "  mpl.rcParams['figure.dpi'] = 300\n",
        "  plt.rcParams[\"font.family\"] = \"serif\"\n",
        "\n",
        "  #Additional step needed to change the labels and title\n",
        "  sanserif = {'fontname':'serif'}\n",
        "\n",
        "  #Sets size, defines 3d plot\n",
        "  fig1=plt.figure(figsize=(20,15))\n",
        "  ax = plt.axes(projection='3d')\n",
        "  \n",
        "  #Plots all the 3D data to a surface plot\n",
        "  #Sets all labels and title with desired font size and label padding, and viewing angle\n",
        "  ax.plot_surface(A_data['Max Depth'], A_data['Number of Trees'], A_data['MAE'], rstride=1, cstride=1, \n",
        "                  cmap='viridis', edgecolor='none', alpha =a_alpha, antialiased=True)\n",
        "  ax.plot_surface(B_data['Max Depth'], B_data['Number of Trees'], B_data['MAE'], rstride=1, cstride=1, \n",
        "                  cmap='viridis', edgecolor='none', alpha =b_alpha, antialiased=True)\n",
        "  ax.plot_surface(C_data['Max Depth'], C_data['Number of Trees'], C_data['MAE'], rstride=1, cstride=1, \n",
        "                  cmap='viridis', edgecolor='none', alpha =c_alpha, antialiased=True)\n",
        "  #Checks to see if there is any data in D_data or E_data, and plots a surface if there is\n",
        "  if D_data != {}:\n",
        "    ax.plot_surface(D_data['Max Depth'], D_data['Number of Trees'], D_data['MAE'], rstride=1, cstride=1, \n",
        "                    cmap='viridis', edgecolor='none', alpha =d_alpha, antialiased=True)\n",
        "    ax.text(min(D_data['Max Depth'].flatten()) - min(D_data['Max Depth'].flatten())*.01, \n",
        "            max(D_data['Number of Trees'].flatten()), \n",
        "            min(D_data['MAE'].flatten()) + min(D_data['MAE'].flatten()) *.005 + d_label_sep,\n",
        "            D_label, fontsize = 12, style='italic')\n",
        "  if E_data != {}:\n",
        "    ax.plot_surface(E_data['Max Depth'], E_data['Number of Trees'], E_data['MAE'], rstride=1, cstride=1, \n",
        "                    cmap='viridis', edgecolor='none', alpha =e_alpha, antialiased=True)\n",
        "    ax.text(min(E_data['Max Depth'].flatten()) - min(E_data['Max Depth'].flatten())*.01, \n",
        "            max(E_data['Number of Trees'].flatten()), \n",
        "            min(E_data['MAE'].flatten()) + min(E_data['MAE'].flatten()) *.005 + e_label_sep,\n",
        "            E_label, fontsize = 12, style='italic')\n",
        "  #Organizes all the formatting, such as the plot title, axis titles, plot annotations\n",
        "  ax.set_title(title + ' Surface Plot | Optimization of Random Forest Regression Model', fontdict = sanserif, fontsize=16, pad=60)\n",
        "  ax.set_ylabel('Number of Trees', fontdict = sanserif, fontsize= 12, labelpad=10)\n",
        "  ax.set_zlabel('Mean Absolute Error', fontdict = sanserif, fontsize=12, labelpad=10)\n",
        "  ax.set_xlabel('Max Depth of Each Tree', fontdict = sanserif, fontsize=12, labelpad=10)\n",
        "  ax.text(min(A_data['Max Depth'].flatten()) - min(A_data['Max Depth'].flatten())*.01, \n",
        "          max(A_data['Number of Trees'].flatten()), \n",
        "          min(A_data['MAE'].flatten()) + min(A_data['MAE'].flatten()) *.005 + a_label_sep,\n",
        "          A_label, fontsize = 12, style='italic')\n",
        "  ax.text(min(B_data['Max Depth'].flatten()) - min(B_data['Max Depth'].flatten())*.01, \n",
        "          max(B_data['Number of Trees'].flatten()), \n",
        "          min(B_data['MAE'].flatten()) + min(B_data['MAE'].flatten()) *.005 + b_label_sep,\n",
        "          B_label, fontsize = 12, style='italic')  \n",
        "  ax.text(min(C_data['Max Depth'].flatten()) - min(C_data['Max Depth'].flatten())*.01, \n",
        "          max(C_data['Number of Trees'].flatten()), \n",
        "          min(C_data['MAE'].flatten()) + min(C_data['MAE'].flatten()) *.005 + c_label_sep,\n",
        "          C_label, fontsize = 12, style='italic')  \n",
        "  ax.view_init(50, 35)\n",
        "def multicomparison_interactive_surface_Forest(title, A_data, B_data, C_data, \n",
        "                                               a_alpha = .8, b_alpha = .8, c_alpha = .8,\n",
        "                                               A_label = '', B_label = '', C_label = '',\n",
        "                                               a_label_sep = 0, b_label_sep = 0, c_label_sep = 0,\n",
        "                                               D_data = {}, d_alpha = .8, D_label = '', d_label_sep = 0,\n",
        "                                               E_data = {}, e_alpha = .8, E_label = '', e_label_sep = 0):\n",
        "  \"\"\"\n",
        "  Allows multiple intereactive surface plots (5 Max) with the desired formatting based on the data inputted.\n",
        "  Check the sister method comparison_interactive_surface_Forest for comparisons with only two datsets.\n",
        "  Note that here dictionaries will be the arguments rather than the values of the dictionaries'\n",
        "  being directly entered, so data returned from a for_plot can be plotted with a single arguemnt.\n",
        "  Arguments with a letter_alpha control transparency of juxtaposed graphs, letter_label create \n",
        "  interactive annotations to help to distinguish between groups of data, and letter_label_sep \n",
        "  offers limited control over separating labels that materialize to close to one another.\n",
        "\n",
        "  Dependencies\n",
        "  *** plotly.graph_objects as go ***\n",
        "  \"\"\"\n",
        "  #Creates surface figure based on data arguments\n",
        "  fig = go.Figure(data=[go.Surface(z = A_data['MAE'], x = A_data['Max Depth'], y = A_data['Number of Trees'], \n",
        "                                   opacity=a_alpha),\n",
        "                        go.Surface(z = B_data['MAE'], x = B_data['Max Depth'], y = B_data['Number of Trees'], \n",
        "                                   opacity=b_alpha, showscale=False),\n",
        "                        go.Surface(z = C_data['MAE'], x = C_data['Max Depth'], y = C_data['Number of Trees'], \n",
        "                                   opacity=c_alpha, showscale=False)\n",
        "                        ])\n",
        "  #Creates dynamic data labels that will be added with update_layout\n",
        "  data_labels = [dict(showarrow=False,\n",
        "                      x=min(A_data['Max Depth'].flatten()),\n",
        "                      y=max(A_data['Number of Trees'].flatten()),\n",
        "                      z=min(A_data['MAE'].flatten()),\n",
        "                      text=A_label,\n",
        "                      xanchor=\"left\",\n",
        "                      xshift=min(A_data['Max Depth'].flatten())*.05,\n",
        "                      yshift=min(A_data['MAE'].flatten())*.001 + a_label_sep/100,\n",
        "                      opacity=0.7),\n",
        "                 dict(showarrow=False,\n",
        "                      x=min(B_data['Max Depth'].flatten()),\n",
        "                      y=max(B_data['Number of Trees'].flatten()),\n",
        "                      z=min(B_data['MAE'].flatten()),\n",
        "                      text=B_label,\n",
        "                      xanchor=\"left\",\n",
        "                      xshift=min(B_data['Max Depth'].flatten())*.05,\n",
        "                      yshift=min(B_data['MAE'].flatten())*.001 + b_label_sep/100,\n",
        "                      opacity=0.7),\n",
        "                 dict(showarrow=False,\n",
        "                      x=min(C_data['Max Depth'].flatten()),\n",
        "                      y=max(C_data['Number of Trees'].flatten()),\n",
        "                      z=min(C_data['MAE'].flatten()),\n",
        "                      text=C_label,\n",
        "                      xanchor=\"left\",\n",
        "                      xshift=min(C_data['Max Depth'].flatten())*.05,\n",
        "                      yshift=min(C_data['MAE'].flatten())*.001 + c_label_sep/100,\n",
        "                      opacity=0.7)\n",
        "                ]\n",
        "  #Checks to see if there is anymore datasets, and adds those datasets\n",
        "  if D_data != {}:\n",
        "      fig.add_trace(go.Surface(z = D_data['MAE'], x = D_data['Max Depth'], y = D_data['Number of Trees'], \n",
        "                               opacity=d_alpha, showscale=False))\n",
        "      data_labels.append(dict(showarrow=False,\n",
        "                              x=min(D_data['Max Depth'].flatten()),\n",
        "                              y=max(D_data['Number of Trees'].flatten()),\n",
        "                              z=min(D_data['MAE'].flatten()),\n",
        "                                text=D_label,\n",
        "                              xanchor=\"left\",\n",
        "                              xshift=min(D_data['Max Depth'].flatten())*.05,\n",
        "                              yshift=min(D_data['MAE'].flatten())*.001 + d_label_sep/100,\n",
        "                              opacity=0.7\n",
        "                          ))\n",
        "  if E_data != {}:\n",
        "      fig.add_trace(go.Surface(z = E_data['MAE'], x = E_data['Max Depth'], y = E_data['Number of Trees'], \n",
        "                               opacity=e_alpha, showscale=False))\n",
        "      data_labels.append(dict(showarrow=False,\n",
        "                              x=min(E_data['Max Depth'].flatten()),\n",
        "                              y=max(E_data['Number of Trees'].flatten()),\n",
        "                              z=min(E_data['MAE'].flatten()),\n",
        "                                text=E_label,\n",
        "                              xanchor=\"left\",\n",
        "                              xshift=min(E_data['Max Depth'].flatten())*.05,\n",
        "                              yshift=min(E_data['MAE'].flatten())*.001 + e_label_sep/100,\n",
        "                              opacity=0.7\n",
        "                          ))                        \n",
        "  #Formats figure title, axis titles, dimesnions, and margins\n",
        "  fig.update_layout(title= title + ' Interactive Surface Plot | Optimization of Random Forest Regression Model', autosize=True,\n",
        "                      scene = dict(xaxis_title='Max Depth of Each Tree',\n",
        "                                  yaxis_title='Number of Trees',\n",
        "                                  zaxis_title='Mean Absolute Error',\n",
        "                                  annotations= data_labels\n",
        "                                  ),\n",
        "                      width=1100, height=800,\n",
        "                      margin=dict(l=65, r=50, b=65, t=90),\n",
        "                   )\n",
        "  fig.show()\n",
        "\n",
        "### Abbreviated Functions\n",
        "### ---------------------\n",
        "### For information consult Section II on Taxonomy of Functions\n",
        "def m_cpFo(title, A_data, B_data, C_data, a_alpha = .8, b_alpha = .8, c_alpha = .8, \n",
        "           A_label = '', B_label = '', C_label = '', a_label_sep = 0, b_label_sep = 0, \n",
        "           c_label_sep = 0, D_data = {}, d_alpha = .8, D_label = '', d_label_sep = 0, \n",
        "           E_data = {}, e_alpha = .8, E_label = '', e_label_sep = 0):\n",
        "  \"\"\"\n",
        "  Consult multicomparison_plot_surface_Forest for documentation.\n",
        "  \"\"\"\n",
        "  return multicomparison_plot_surface_Forest(title, A_data, B_data, C_data, a_alpha = a_alpha, b_alpha = b_alpha, c_alpha = c_alpha, \n",
        "                                             A_label = A_label, B_label = B_label, C_label = C_label, a_label_sep = a_label_sep, b_label_sep = b_label_sep, \n",
        "                                             c_label_sep = c_label_sep, D_data = D_data, d_alpha = d_alpha, D_label = D_label, d_label_sep = d_label_sep, \n",
        "                                             E_data = E_data, e_alpha = e_alpha, E_label = E_label, e_label_sep = e_label_sep)\n",
        "def m_ciFo(title, A_data, B_data, C_data, a_alpha = .8, b_alpha = .8, c_alpha = .8, \n",
        "           A_label = '', B_label = '', C_label = '', a_label_sep = 0, b_label_sep = 0, \n",
        "           c_label_sep = 0, D_data = {}, d_alpha = .8, D_label = '', d_label_sep = 0, \n",
        "           E_data = {}, e_alpha = .8, E_label = '', e_label_sep = 0):\n",
        "  \"\"\"\n",
        "  Consult multicomparison_plot_surface_Forest for documentation.\n",
        "  \"\"\"\n",
        "  return multicomparison_interactive_surface_Forest(title, A_data, B_data, C_data, a_alpha = a_alpha, b_alpha = b_alpha, c_alpha = c_alpha, \n",
        "                                                    A_label = A_label, B_label = B_label, C_label = C_label, a_label_sep = a_label_sep, b_label_sep = b_label_sep, \n",
        "                                                    c_label_sep = c_label_sep, D_data = D_data, d_alpha = d_alpha, D_label = D_label, d_label_sep = d_label_sep, \n",
        "                                                    E_data = E_data, e_alpha = e_alpha, E_label = E_label, e_label_sep = e_label_sep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG-H84_V5jo4",
        "colab_type": "text"
      },
      "source": [
        "#### Functions from Section C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukg90q83-Vyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mae(model_arg, X_arg, y_arg):\n",
        "  \"\"\"\n",
        "  Performs the same function as the mean_absolute_error with the addition of a\n",
        "  model argument that incoporates the train_model function to return the\n",
        "  mean absolute error (MAE). Note K-fold validation was added to later\n",
        "  functions.\n",
        "\n",
        "  mae(my_model, predictors, target_variable)\n",
        "  >>> returns mean_absolute_error(pred_y, val_y)\n",
        "  \"\"\"\n",
        "  pred_y, val_y = train_model(model_arg, X_arg, y_arg)\n",
        "  mae = mean_absolute_error(pred_y, val_y)\n",
        "  return mae\n",
        "\n",
        "def mae_cross_val(model_arg, X_arg, y_arg, cv_arg = 5):\n",
        "  \"\"\"\n",
        "  Performs the same function as the cross_val_score, and then returns the mean of the K_fold validation with\n",
        "  cv_arg folds, which defaults to 5 if none is specified, as a postive floating point. The standard deviation\n",
        "  of all the folds is also returned.\n",
        "\n",
        "  *** numpy as np is a dependency ***\n",
        "  *** cross_val_score from sklearn.model_selection is dependency ***\n",
        "  \"\"\"\n",
        "  mae_result_array = abs(cross_val_score(model_arg, X_arg, y_arg, scoring = 'neg_mean_absolute_error', cv = cv_arg))\n",
        "  mae_result_mean = mae_result_array.mean()\n",
        "  mae_result_sd = np.std(mae_result_array)\n",
        "  return mae_result_mean, mae_result_sd\n",
        "def initialize_Pipeline(preprocessor_arg, model_arg):\n",
        "  \"\"\"\n",
        "  Initializes a Pipeline with given preprocessing method and ML model.\n",
        "  \"\"\"\n",
        "  pipeline = Pipeline(steps=[('preprocessor', preprocessor_arg),\n",
        "                             ('model', model_arg)])\n",
        "  return pipeline\n",
        "def experiment_with_pipelineCV_Forest(n_estimators_range_arg, max_depth_num_or_list_arg, X_arg, y_arg, preprocessor_arg,\n",
        "                                      NE_increment = 1, cv = 5, percent_complete = False):\n",
        "  \"\"\"\n",
        "  Tests a Random Forest Regerssor model with data that encoded and imputed through a pipeline object.\n",
        "  All the arguments of experiment_with_Forest apply except for the isDataTable boolean, as all future\n",
        "  data will be converted to a data table after the speed results found in Subsection D - Comparison\n",
        "  with Gridsearch were comparable with the intended optimization output style. Additionally, there\n",
        "  is the additonal preprocessor_arg and cv argument which is designated respectively for a ColumnTransformer\n",
        "  and number of K-folds. If cv is set to 1, then only the mae_results without teh mae_results_sd, otherwise\n",
        "  both mae_results and mae_results_sd (which represents the standard deviation of the K-fold validation), will\n",
        "  be returned.\n",
        "  \"\"\"\n",
        "  #Initializes all the needed dictionaries and ranges\n",
        "  mae_results = {}\n",
        "  mae_results_sd = {}\n",
        "  n_estimators_range = list(range(1, n_estimators_range_arg+1, NE_increment))\n",
        "  max_depth_range = max_depth_num_or_list_arg\n",
        "  if percent_complete:\n",
        "    #Finds total number of iterations needed to complete function\n",
        "    total_num_of_iterations = len(max_depth_range)*len(n_estimators_range)\n",
        "  for max_depth in max_depth_range:\n",
        "    #Now max_depth will be constant while it runs through all the n_estimators, then it will increase by 1 and iterate\n",
        "    for n_estimators in n_estimators_range:\n",
        "      #Intitializes model and pipeline with given preprocessor, calculates MAE with dataset \n",
        "      Forest_model = initialize_Forest(n_estimators, max_depth)\n",
        "      pipeline_model = initialize_Pipeline(preprocessor_arg, Forest_model)\n",
        "      #If the number of K-folds is greater than 1, both the average of the folds and standard deviation is returned\n",
        "      if cv > 1:\n",
        "        mae_res, mae_res_sd = mae_cross_val(pipeline_model, X_arg, y_arg, cv_arg = cv)\n",
        "        mae_results.setdefault(max_depth, []).append([max_depth, mae_res, n_estimators])\n",
        "        mae_results_sd.setdefault(max_depth, []).append([n_estimators, mae_res_sd])\n",
        "      #Otherwise, only the result of the one fold is calculated\n",
        "      else:\n",
        "        mae_res = mae(pipeline_model, X_arg, y_arg)\n",
        "        mae_results.setdefault(max_depth, []).append([max_depth, mae_res, n_estimators])\n",
        "      if percent_complete:\n",
        "        #Finds the number of iterations completed by looking at indexes of n_estimators and max_depth\n",
        "        num_of_iterations_completed = (n_estimators_range.index(n_estimators)+1) + (max_depth_range.index(max_depth)*len(n_estimators_range))\n",
        "        num_of_iterations_completed_bef = (n_estimators_range.index(n_estimators)) + (max_depth_range.index(max_depth)*len(n_estimators_range))\n",
        "        #Calculates percentage of function done, total_num_of_iterations defined at the beginning of else statement\n",
        "        progress_percent = round((num_of_iterations_completed / total_num_of_iterations), 3) * 100\n",
        "        progress_percent_bef = round((num_of_iterations_completed_bef / total_num_of_iterations), 3) * 100\n",
        "        #Removes print if progress_precent hasn't changed\n",
        "        if (progress_percent != progress_percent_bef):\n",
        "          print(str(progress_percent) +'%')\n",
        "  if cv > 1:\n",
        "    return mae_results, mae_results_sd\n",
        "  else:\n",
        "    return mae_results\n",
        "def average_SD_pipelineCV_Forest(mae_results_sd):\n",
        "  \"\"\"\n",
        "  Finds the average mae value at a specifc n_estimators over all max_depth values.\n",
        "  Returns a DataFrame with the columns 'n_estimators' and 'Standard Deviation'.\n",
        "\n",
        "  *** Numpy as np and Pandas as pd dependencies ***\n",
        "  \"\"\"\n",
        "  #Finds the values, converts to a numpy array, and initializes return dictionary\n",
        "  (key_values, data_unzipped) = zip(*mae_results_sd.items())\n",
        "  raw_sd = np.array(data_unzipped)\n",
        "  pruned_sd = {'n_estimators':[], 'Standard Deviation':[]}\n",
        "  #Iterates through rows, which are separated based on n_estimators\n",
        "  for n_estimators in range(0, len(raw_sd[0])):\n",
        "    #Finds average mean across all max_depth arrays with constant n_estimators\n",
        "    avg_at_n_estimators = raw_sd[:,n_estimators,1].mean()\n",
        "    #Adds the n_estimators and respective average mae to dictionary\n",
        "    pruned_sd['n_estimators'].append(raw_sd[0,n_estimators,0])\n",
        "    pruned_sd['Standard Deviation'].append(avg_at_n_estimators)\n",
        "  #Converts to a DataFrame with both columns set as an index\n",
        "  return pd.DataFrame(pruned_sd, index = None)\n",
        "\n",
        "### Abbreviated Functions\n",
        "### ---------------------\n",
        "### For information consult Section II on Taxonomy of Functions\n",
        "def ewpi(n_estimators_range_arg, max_depth_num_or_list_arg, X_arg, y_arg, \n",
        "                                      preprocessor_arg, NE_increment = 1, cv = 5, percent_complete = False):\n",
        "  \"\"\"\n",
        "  Consult experiment_with_pipelineCV_Forest for documentation.\n",
        "  \"\"\"\n",
        "  return experiment_with_pipelineCV_Forest(n_estimators_range_arg, max_depth_num_or_list_arg, X_arg, y_arg, \n",
        "                                           preprocessor_arg, NE_increment = NE_increment, cv = cv, percent_complete = percent_complete)\n",
        "def iPi(preprocessor_arg, model_arg):\n",
        "  \"\"\"\n",
        "  Consult initialize_Pipeline for documentation.\n",
        "  \"\"\"\n",
        "  return initialize_Pipeline(preprocessor_arg, model_arg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIJHahyQlHe8",
        "colab_type": "text"
      },
      "source": [
        "### E. Functions from Section IV. XGBoost Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a232yZ43lWkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def experiment4D_with_XGB(n_estimators_range_arg, max_depth_num_or_list_arg, learning_rate_range_arg, X_arg, y_arg, preprocessor_arg,\n",
        "                                      NE_increment = 1, cv = 5, percent_complete = False, learning_rate_divisor = 100):\n",
        "  \"\"\"\n",
        "  Enables trivariate hyperparameter optimization of a XGB Regressor. The first three arguments define the range\n",
        "  of the n_estimators, max_depth, and learning_rate. The next two define data, and the next the preprocessor/\n",
        "  ColumnTransformer object. The NE_increment changes the increment of n_estimators as the range argument is\n",
        "  an int. The cv determines number of K-folds. Percent_complete offers console dialog on the status of the\n",
        "  function. Learning_rate_divisor is what the range entered in the learning_rate_range argument is divided\n",
        "  by; for instance, range(1,100) with the default learning_rate_divisor would test a learning rate of \n",
        "  .01 to .99.\n",
        "  \"\"\"\n",
        "  #Sets the ranges of the hyperparamters\n",
        "  mae_results = {}\n",
        "  mae_results_sd = {}\n",
        "  n_estimators_range = list(range(1, n_estimators_range_arg+1, NE_increment))\n",
        "  max_depth_range = max_depth_num_or_list_arg\n",
        "  learning_rate_range = [rate / learning_rate_divisor for rate in learning_rate_range_arg]\n",
        "  if percent_complete:\n",
        "    #Finds total number of iterations needed to complete function\n",
        "    total_num_of_iterations = len(max_depth_range)*len(n_estimators_range)*len(learning_rate_range)\n",
        "  for max_depth in max_depth_range:\n",
        "    #Now max_depth will be constant while it runs through all the n_estimators, then it will increase by 1 and iterate\n",
        "    for learning_rate in learning_rate_range:\n",
        "      for n_estimators in n_estimators_range:\n",
        "        #Intitializes model and pipeline with given preprocessor, calculates MAE with dataset, \n",
        "        XGB_model = initialize_XGB(n_estimators, learning_rate, max_depth)\n",
        "        pipeline_model = initialize_Pipeline(preprocessor_arg, XGB_model)\n",
        "        #Determines whether cross-validation is performed\n",
        "        if cv > 1:\n",
        "          mae_res, mae_res_sd = mae_cross_val(pipeline_model, X_arg, y_arg, cv_arg = cv)\n",
        "          mae_results.setdefault(max_depth, []).append([max_depth, mae_res, n_estimators, learning_rate])\n",
        "          mae_results_sd.setdefault(max_depth, []).append([n_estimators, learning_rate, mae_res_sd])\n",
        "        else:\n",
        "          mae_res = mae(pipeline_model, X_arg, y_arg)\n",
        "          mae_results.setdefault(max_depth, []).append([max_depth, mae_res, n_estimators, learning_rate])\n",
        "        #Calculates how many iterations are completed and how many total, returning the ratio as a percentage\n",
        "        if percent_complete:\n",
        "          #Finds the number of iterations completed by looking at indexes of n_estimators, max_depth, leanring_rate\n",
        "          max_depth_loop_num = len(n_estimators_range)*len(learning_rate_range)*max_depth_range.index(max_depth)\n",
        "          learning_rate_num = len(n_estimators_range)*learning_rate_range.index(learning_rate)\n",
        "          num_of_iterations_completed = n_estimators_range.index(n_estimators) + 1 + learning_rate_num + max_depth_loop_num\n",
        "          num_of_iterations_completed_bef = n_estimators_range.index(n_estimators) + learning_rate_num + max_depth_loop_num\n",
        "          #Calculates percentage of function done, total_num_of_iterations defined at the beginning of else statement\n",
        "          progress_percent = round((num_of_iterations_completed / total_num_of_iterations), 3) * 100\n",
        "          #Calculates the percentage before\n",
        "          progress_percent_bef = round((num_of_iterations_completed_bef / total_num_of_iterations), 3) * 100\n",
        "          #Removes print if progress_precent hasn't changed\n",
        "          if (progress_percent != progress_percent_bef):\n",
        "            print(str(progress_percent) +'%')\n",
        "  #Returns the standard deviation results if cross-validation was performed\n",
        "  if cv > 1:\n",
        "    return mae_results, mae_results_sd\n",
        "  else:\n",
        "    return mae_results\n",
        "def for_4D_plot_XGB(experiment_with_XGB_res):\n",
        "  \"\"\"\n",
        "  This converts the data to a format that can be used for a four dimensional\n",
        "  graph. The format is a dictionary with the keys 'max_depth', 'mae', 'n_estimators',\n",
        "  'learning_rate' and two-dimensional matrices stored in the values. It searches for\n",
        "  the lowest max_depth value acros data with the same learning_rate and n_estimators\n",
        "  and creates a dataset based on all of these values.\n",
        "  \"\"\"\n",
        "  #Locates all the values and places them into a numpy array\n",
        "  (key_values, data_unzipped) = zip(*experiment_with_XGB_res.items())\n",
        "  raw_4D = np.array(data_unzipped)\n",
        "\n",
        "  #List will \"shuttle values\" to plottable_4D before being erased after each for loop\n",
        "  shuttle_list = []\n",
        "  #Stores all the results from shuttle_list\n",
        "  plottable_4D = []\n",
        "  #Program will recognize the number of unique n_estimators\n",
        "  unique_NE = 1\n",
        "  #Finds how many unique instances of n_estimators without needing a user input\n",
        "  while raw_4D[0,0,2] != raw_4D[0, unique_NE,2]:\n",
        "    unique_NE += 1\n",
        "  #Iterates through all the data collected in the experiment_with_XGB\n",
        "  for t in range(0, len(raw_4D[0])):\n",
        "    #Locates the lowest min value with learning_rate and n_estimators constant, but max_depth varies\n",
        "    min_mae = min(raw_4D[:,t,1])\n",
        "    #Finds the indices of this mae value \n",
        "    i, j, k = np.where(raw_4D == min_mae)\n",
        "    #If there two or more mae values that are equally the lowest, then searches for lowest max_depth\n",
        "    if len(i) > 1:\n",
        "      jk_index = 0\n",
        "      still_searching = True\n",
        "      for i_index in range(0, len(i)):\n",
        "        if still_searching:\n",
        "          if raw_4D[i[i_index], j[jk_index], k[jk_index]] in raw_4D[i,j,:]:\n",
        "            i, j, k = i[i_index], j[jk_index], k[jk_index]\n",
        "            still_searching = False\n",
        "          jk_index += 1\n",
        "    #Adds this mae value with the lowest max_depth to the shuttle\n",
        "    shuttle_list.append(list(raw_4D[i,j,:].flatten()))\n",
        "    #When all the n_estimators vals in the range have been searched, the shuttle list drops it off at plottable_4D\n",
        "    if (unique_NE - 1) == (t % unique_NE):\n",
        "      plottable_4D.append(shuttle_list)\n",
        "      shuttle_list = []\n",
        "  #Following a conversion to a numpy, values will be transposed to enable it to graphed in a 3D plot, and these values will be stored in a dictionary\n",
        "  pruned_4D = np.array(plottable_4D)\n",
        "  max_depth_vals = pruned_4D[0:,0:,0].T\n",
        "  mae_vals = pruned_4D[0:,0:,1].T\n",
        "  n_estimators_vals = pruned_4D[0:,0:,2].T\n",
        "  learning_rate_vals = pruned_4D[0:,0:,3].T\n",
        "  return {'max_depth':max_depth_vals,'mae':mae_vals,'n_estimators':n_estimators_vals,'learning_rate':learning_rate_vals}\n",
        "\n",
        "def optimize_XGB(DataTable, is_4D = False, test_max_depth = True):\n",
        "  \"\"\"\n",
        "  Returns the lowest MAE value and parameters attendant including learning_rate, n_estimators, and max_depth.\n",
        "  If the function experiment4D_with_XGB was used, set is_4D to True, otherwise simply insert what was returned\n",
        "  from experiment_with_XGB function.\n",
        "  \"\"\"\n",
        "  #Converts dictionary to list of mae values (mae_results)\n",
        "  all_vals = np.array(list(DataTable.values()))\n",
        "  mae_results = list(all_vals[0:,0:,1].flatten())\n",
        "  max_depth_results = list(all_vals[0:,0:,0].flatten())\n",
        "  n_estimators_results = list(all_vals[0:,0:,2].flatten())\n",
        "  learning_rate_results = list(all_vals[0:,0:,3].flatten())\n",
        "\n",
        "  #Finds the loewst MAE value in the list of mae_results\n",
        "  min_mae = min(mae_results)\n",
        "  #Locates the index of the lowest MAE value\n",
        "  min_mae_index = mae_results.index(min_mae)\n",
        "  opt_n_estimators = n_estimators_results[min_mae_index]\n",
        "  opt_max_depth = max_depth_results[min_mae_index]\n",
        "  opt_learning_rate = learning_rate_results[min_mae_index]\n",
        "  \n",
        "  #Sorts optimization results based on experiment\n",
        "  if is_4D:\n",
        "    return {'Optimal max_depth':int(opt_max_depth), 'Optimal n_estimators':int(opt_n_estimators), 'Optimal MAE':min_mae,\n",
        "          'Optimal learning_rate':opt_learning_rate}\n",
        "  elif test_max_depth:\n",
        "    return {'Optimal max_depth':int(opt_max_depth), 'Optimal n_estimators':int(opt_n_estimators), \n",
        "            'Optimal MAE':min_mae, 'Constant learning_rate':opt_learning_rate}\n",
        "  else:\n",
        "    return {'Constant max_depth':int(opt_max_depth), 'Optimal n_estimators':int(opt_n_estimators), \n",
        "            'Optimal MAE':min_mae, 'Optimal learning_rate':opt_learning_rate}\n",
        "def for_3D_plot_XGB(experiment_with_XGB_res, test_max_depth = True):\n",
        "  \"\"\"\n",
        "  Converts the data from the function experiment_with_XGB into plottable 3D data. If the function\n",
        "  tested n_estimators over a changing learning_rate instead of a max_depth, then make sure to \n",
        "  change test_max_depth to False. For the most comprehensive documentation on the function's \n",
        "  modus operandi consult for_3D_plot_Forest.\n",
        "  \"\"\"\n",
        "  #Separates the keys from the values, remember all the needed data is in the values\n",
        "  #Needed data = [max_depth,mae_res,n_estimators]\n",
        "  (key_values, data_unzipped) = zip(*experiment_with_XGB_res.items())\n",
        "  #Converted data_unzipped to an numpy array, where then data is sliced to max_depth\n",
        "  #mae_res,n_estimators accordingly, transposed, and then stored accordingly\n",
        "  data_array = np.array(data_unzipped)\n",
        "  max_depth_or_learning_rate_vals = data_array[0:,0:,0].T\n",
        "  mae_vals = data_array[0:,0:,1].T\n",
        "  n_estimators_vals = data_array[0:,0:,2].T\n",
        "  if test_max_depth:\n",
        "    A_data = {'Max Depth': max_depth_or_learning_rate_vals, 'MAE':mae_vals, 'Number of Trees':n_estimators_vals}\n",
        "  else:\n",
        "    A_data = {'Learning Rate': max_depth_or_learning_rate_vals, 'MAE':mae_vals, 'Number of Trees':n_estimators_vals}\n",
        "  return A_data\n",
        "def interactive_surface_XGB(title, A_data):\n",
        "  \"\"\"\n",
        "  ****\n",
        "  Creates an interactive surface plot with the desired formatting based on the data inputted.\n",
        "\n",
        "  Dependencies\n",
        "  *** plotly.graph_objects as go ***\n",
        "  \"\"\"\n",
        "  #Checks to see which parameter, either max_depth or learning_rate\n",
        "  if 'Max Depth' in A_data.keys():\n",
        "    #Creates surface figure based on data arguments\n",
        "    fig = go.Figure(data=[go.Surface(z = A_data['MAE'], x = A_data['Max Depth'], y = A_data['Number of Trees'])])\n",
        "    #Formats figure title, axis titles, dimesnions, and margins\n",
        "    fig.update_layout(title= title + ' Interactive Surface Plot | Optimization of XGBR Model', autosize=True,\n",
        "                      scene = dict(\n",
        "                      xaxis_title='Max Depth of Each Tree',\n",
        "                      yaxis_title='Number of Trees',\n",
        "                      zaxis_title='Mean Absolute Error'),\n",
        "                      width=1100, height=800,\n",
        "                      margin=dict(l=65, r=50, b=65, t=90))\n",
        "  else: \n",
        "    #Creates surface figure based on data arguments\n",
        "    fig = go.Figure(data=[go.Surface(z = A_data['MAE'], x = A_data['Learning Rate'], y = A_data['Number of Trees'])])\n",
        "    #Formats figure title, axis titles, dimesnions, and margins\n",
        "    fig.update_layout(title= title + ' Interactive Surface Plot | Optimization of XGBR Model', autosize=True,\n",
        "                      scene = dict(\n",
        "                      xaxis_title='Learning Rate',\n",
        "                      yaxis_title='Number of Trees',\n",
        "                      zaxis_title='Mean Absolute Error'),\n",
        "                      width=1100, height=800,\n",
        "                      margin=dict(l=65, r=50, b=65, t=90))    \n",
        "  fig.show()\n",
        "def comparison_plot_surface_XGB(title, A_data, B_data, a_alpha = .8, b_alpha = .8,\n",
        "                                   A_label = '', B_label = '', a_label_sep = 0, b_label_sep = 0,\n",
        "                                   a_xshift = 0, b_xshift = 0):\n",
        "  \"\"\"\n",
        "  Plots two surface plots with the desired formatting based on the data inputted.\n",
        "\n",
        "  Dependencies\n",
        "  *** matplotlib as mpl ***\n",
        "  *** matplotlib.pyplot as plt ***\n",
        "  \"\"\"\n",
        "  #Sets parameters for figure quality and text font (for consistency across notebook)\n",
        "  mpl.rcParams['figure.dpi'] = 300\n",
        "  plt.rcParams[\"font.family\"] = \"serif\"\n",
        "\n",
        "  #Additional step needed to change the labels and title\n",
        "  sanserif = {'fontname':'serif'}\n",
        "\n",
        "  #Sets size, defines 3d plot\n",
        "  fig1=plt.figure(figsize=(20,15))\n",
        "  ax = plt.axes(projection='3d')\n",
        "  \n",
        "  #Plots all the 3D data to a surface plot\n",
        "  #Sets all labels and title with desired font size and label padding, and viewing angle\n",
        "  ax.plot_surface(A_data['Max Depth'], A_data['Number of Trees'], A_data['MAE'], rstride=1, cstride=1, \n",
        "                  cmap='viridis', edgecolor='none', alpha =a_alpha, antialiased=True)\n",
        "  ax.plot_surface(B_data['Max Depth'], B_data['Number of Trees'], B_data['MAE'], rstride=1, cstride=1, \n",
        "                  cmap='viridis', edgecolor='none', alpha =b_alpha, antialiased=True)\n",
        "  ax.set_title(title + ' Surface Plot | Optimization of XGBR Model', fontdict = sanserif, fontsize=16, pad=60)\n",
        "  ax.set_ylabel('Number of Trees', fontdict = sanserif, fontsize= 12, labelpad=10)\n",
        "  ax.set_zlabel('Mean Absolute Error', fontdict = sanserif, fontsize=12, labelpad=10)\n",
        "  ax.set_xlabel('Max Depth of Each Tree', fontdict = sanserif, fontsize=12, labelpad=10)\n",
        "  ax.text(min(A_data['Max Depth'].flatten()) - min(A_data['Max Depth'].flatten())*.01, \n",
        "          max(A_data['Number of Trees'].flatten()) + a_xshift, \n",
        "          min(A_data['MAE'].flatten()) + min(A_data['MAE'].flatten()) *.005 + a_label_sep,\n",
        "          A_label, fontsize = 12, style='italic')\n",
        "  ax.text(min(B_data['Max Depth'].flatten()) - min(B_data['Max Depth'].flatten())*.01, \n",
        "          max(B_data['Number of Trees'].flatten()) + b_xshift, \n",
        "          min(B_data['MAE'].flatten()) + min(B_data['MAE'].flatten()) *.005 + b_label_sep,\n",
        "          B_label, fontsize = 12, style='italic')  \n",
        "  ax.view_init(50, 35)\n",
        "def comparison_interactive_surface_XGB(title, A_data, B_data, a_alpha = .8, b_alpha = .8,\n",
        "                                       A_label = '', B_label = '', a_label_sep = 0, b_label_sep = 0):\n",
        "  \"\"\"\n",
        "  Creates two interactive surface plots with the desired formatting based on the data inputted.\n",
        "\n",
        "  Dependencies\n",
        "  *** plotly.graph_objects as go ***\n",
        "  \"\"\"\n",
        "  #Creates surface figure based on data arguments\n",
        "  fig = go.Figure(data=[go.Surface(z = A_data['MAE'], x = A_data['Max Depth'], y = A_data['Number of Trees'], \n",
        "                                   opacity=a_alpha),\n",
        "                        go.Surface(z = B_data['MAE'], x = B_data['Max Depth'], y = B_data['Number of Trees'], \n",
        "                                   opacity=b_alpha, showscale=False)])\n",
        "  #Formats figure title, axis titles, dimesnions, and margins\n",
        "  fig.update_layout(title= title + ' Interactive Surface Plot | Optimization of XGBR Model', autosize=True,\n",
        "                    scene = dict(xaxis_title='Max Depth of Each Tree',\n",
        "                                 yaxis_title='Number of Trees',\n",
        "                                 zaxis_title='Mean Absolute Error',\n",
        "                                 annotations= [dict(showarrow=False,\n",
        "                                                    x=min(A_data['Max Depth'].flatten()),\n",
        "                                                    y=max(A_data['Number of Trees'].flatten()),\n",
        "                                                    z=min(A_data['MAE'].flatten()),\n",
        "                                                    text=A_label,\n",
        "                                                    xanchor=\"left\",\n",
        "                                                    xshift=min(A_data['Max Depth'].flatten())*.05,\n",
        "                                                    yshift=min(A_data['MAE'].flatten())*.001 + a_label_sep/100,\n",
        "                                                    opacity=0.7\n",
        "                                              ),\n",
        "                                              dict(showarrow=False,\n",
        "                                                   x=min(B_data['Max Depth'].flatten()),\n",
        "                                                   y=max(B_data['Number of Trees'].flatten()),\n",
        "                                                   z=min(B_data['MAE'].flatten()),\n",
        "                                                   text=B_label,\n",
        "                                                   xanchor=\"left\",\n",
        "                                                   xshift=min(B_data['Max Depth'].flatten())*.05,\n",
        "                                                   yshift=min(B_data['MAE'].flatten())*.001 + b_label_sep/100,\n",
        "                                                   opacity=0.7\n",
        "                                              )]\n",
        "                                 ),\n",
        "                    width=1100, height=800,\n",
        "                    margin=dict(l=65, r=50, b=65, t=90),\n",
        "                    )\n",
        "  fig.show()\n",
        "def interactive_4Dsurface_XGB(title, A_data):\n",
        "  \"\"\"\n",
        "  Creates an interactive surface plot with the desired formatting based on the data inputted. Expects\n",
        "  a fourth dimension of 'max_depth' and will represent this with color as the z-axis, x-axis, and y-axis\n",
        "  will respectively be 'mae', 'n_estimators', and 'learning_rate\" within the dictionary A_data.\n",
        "\n",
        "  Dependencies\n",
        "  *** plotly.graph_objects as go ***\n",
        "  \"\"\"\n",
        "  #Creates surface figure based on data arguments\n",
        "  fig = go.Figure(data=[go.Surface(z = A_data['mae'], x = A_data['n_estimators'], y = A_data['learning_rate'],\n",
        "                                   surfacecolor = A_data['max_depth'], colorbar = dict(title = 'Max Depth'))])\n",
        "  #Formats figure title, axis titles, dimesnions, and margins\n",
        "  fig.update_layout(title= title + ' - 4D Interactive Surface Plot | Optimization of XGBR Model', autosize=True,\n",
        "                    scene = dict(\n",
        "                      xaxis_title='Number of Trees',\n",
        "                      yaxis_title='Learning Rate',\n",
        "                      zaxis_title='Mean Absolute Error'),\n",
        "                    width=1100, height=800,\n",
        "                    margin=dict(l=65, r=50, b=65, t=90))\n",
        "  fig.show()\n",
        "def initialize_XGB(n_estimators_arg, learning_rate_arg, max_depth_arg):\n",
        "  \"\"\"\n",
        "  Initializes an XGB Regressor to desired n_estimators, learning_rate, and max_depth.\n",
        "  \"\"\"\n",
        "  model = XGBRegressor(n_estimators = n_estimators_arg, learning_rate = learning_rate_arg, max_depth = max_depth_arg,\n",
        "                       n_jobs = 8, random_state = 0, verbosity = 0)\n",
        "  return model\n",
        "def experiment_with_XGB(n_estimators_range_arg, max_depth_or_learning_rate, X_arg, y_arg, preprocessor_arg,\n",
        "                        NE_increment = 1, cv = 5, percent_complete = False, test_max_depth = True,\n",
        "                        learning_rate_divisor = 100, default_learning_rate = .1, default_max_depth = 5):\n",
        "  \"\"\"\n",
        "  Enables bivariate hyperparamter optimization of a XGB Regressor model. For trivariate hyperparamter optimization,\n",
        "  consider using the function experiment4D_with_XGB. This enables the n_estimators to always change and either the\n",
        "  max_depth or learning_rate to change with the opposite remaining constant. To change the constant variable, change\n",
        "  the default_learning_rate or default_max_depth to desired value; if you wish to test max_depth, set test_max_depth\n",
        "  to True, conversely set test_max_depth to False to test learning_rate. For a detailed overview of all arguments,\n",
        "  look below.\n",
        "\n",
        "  Parameters:\n",
        "  * n_estimators_range_arg: same as previous experiment functions---a single integer\n",
        "  * max_depth_or_learning_rate: a range(x,y,z) argument where x is the lower limit, y is the upper limit (not included)\n",
        "    and z is the increment of the range\n",
        "  * X_arg and y_arg: the predictors and target variable\n",
        "  * preprocessor_arg: a ColumnTransformer object\n",
        "  * NE_increment: increment n_estimators_range_arg\n",
        "  * cv: number of K-folds\n",
        "  * percent_complete: reports how much of function is completed\n",
        "  * test_max_depth: defaults to True, if True, max_depth_or_learning_rate range changes the max_depth, otherwise it\n",
        "    changes the the learning_rate\n",
        "  * learning_rate_divisor: divides the range, if testing learning rate (test_max_depth = False), by this value. For\n",
        "    instance, with a range(1,100) and default learning_rate divisor of 100, then learning_rate will be tested from\n",
        "    .01 to .99.\n",
        "  * default_learning_rate: what the learning_rate is if test_max_depth = True\n",
        "  * default_max_depth: what the max_depth is if test_max_depth = False\n",
        "  \"\"\"\n",
        "  #Initializes return dictionaries and sets n_estimators range\n",
        "  mae_results = {}\n",
        "  mae_results_sd = {}\n",
        "  n_estimators_range = list(range(1, n_estimators_range_arg+1, NE_increment))\n",
        "  #Finds total number of iterations\n",
        "  if percent_complete:\n",
        "    total_num_of_iterations = len(max_depth_or_learning_rate)*len(n_estimators_range)\n",
        "  #Runs this if user wishes to test max_depth with a constant learning_rate  \n",
        "  if test_max_depth:\n",
        "    max_depth_range = max_depth_or_learning_rate\n",
        "    for max_depth in max_depth_range:\n",
        "      for n_estimators in n_estimators_range:\n",
        "        #Initializes an XGB model with the default learning_rate and changing max_depth and n_estimators\n",
        "        XGB_model = initialize_XGB(n_estimators, default_learning_rate, max_depth)\n",
        "        #Initializes pipeline with XGB model and preprocessor stated in the argument\n",
        "        pipeline_model = initialize_Pipeline(preprocessor_arg, XGB_model)\n",
        "        #Determines whether K-fold validation will take place, and if so records standard deviation in addition to mae\n",
        "        if cv > 1:\n",
        "          mae_res, mae_res_sd = mae_cross_val(pipeline_model, X_arg, y_arg, cv_arg = cv)\n",
        "          mae_results.setdefault(max_depth, []).append([max_depth, mae_res, n_estimators, default_learning_rate])\n",
        "          mae_results_sd.setdefault(max_depth, []).append([n_estimators, mae_res_sd])\n",
        "        #Runs if there is no K-fold validation\n",
        "        else:\n",
        "          mae_res = mae(pipeline_model, X_arg, y_arg)\n",
        "          mae_results.setdefault(max_depth, []).append([max_depth, mae_res, n_estimators, default_learning_rate])\n",
        "        #Divides the number of iterations of completed by the total number of iterations to return percentage of function completed\n",
        "        if percent_complete:\n",
        "          num_of_iterations_completed = (n_estimators_range.index(n_estimators)+1) + (max_depth_range.index(max_depth)*len(n_estimators_range))\n",
        "          num_of_iterations_completed_bef = (n_estimators_range.index(n_estimators)) + (max_depth_range.index(max_depth)*len(n_estimators_range))\n",
        "          progress_percent = round((num_of_iterations_completed / total_num_of_iterations), 3) * 100\n",
        "          progress_percent_bef = round((num_of_iterations_completed_bef / total_num_of_iterations), 3) * 100\n",
        "          if (progress_percent != progress_percent_bef):\n",
        "            #Prints the total percentage of function completed\n",
        "            print(str(progress_percent) +'%')    \n",
        "  #Runs this if user wishes to test learning_rate with a constant max_depth\n",
        "  else:\n",
        "    learning_rate_range_arg = max_depth_or_learning_rate\n",
        "    #Divides learning_rate_range by the learning_rate_divisor which defaults to 100\n",
        "    #This means that a range of (1,100) with default divisor will test [.01 to .99] learning_rate\n",
        "    learning_rate_range = [rate / learning_rate_divisor for rate in learning_rate_range_arg]\n",
        "    for learning_rate in learning_rate_range:\n",
        "      for n_estimators in n_estimators_range:\n",
        "        #Initializes an XGB model with the default max_depth and changing learning_rate and n_estimators\n",
        "        XGB_model = initialize_XGB(n_estimators, learning_rate, default_max_depth)\n",
        "        #Initializes pipeline with XGB model and preprocessor stated in the argument\n",
        "        pipeline_model = initialize_Pipeline(preprocessor_arg, XGB_model)\n",
        "        #Determines whether K-fold validation will take place, and if so records standard deviation in addition to mae\n",
        "        if cv > 1:\n",
        "          mae_res, mae_res_sd = mae_cross_val(pipeline_model, X_arg, y_arg, cv_arg = cv)\n",
        "          mae_results.setdefault(learning_rate, []).append([default_max_depth, mae_res, n_estimators, learning_rate])\n",
        "          mae_results_sd.setdefault(learning_rate, []).append([n_estimators, mae_res_sd])\n",
        "        #Runs if there is no K-fold validation\n",
        "        else:\n",
        "          mae_res = mae(pipeline_model, X_arg, y_arg)\n",
        "          mae_results.setdefault(learning_rate, []).append([default_max_depth, mae_res, n_estimators, learning_rate])\n",
        "        #Divides the number of iterations of completed by the total number of iterations to return percentage of function completed\n",
        "        if percent_complete:\n",
        "          num_of_iterations_completed = (n_estimators_range.index(n_estimators)+1) + (learning_rate_range.index(learning_rate)*len(n_estimators_range))\n",
        "          num_of_iterations_completed_bef = (n_estimators_range.index(n_estimators)) + (learning_rate_range.index(learning_rate)*len(n_estimators_range))\n",
        "          progress_percent = round((num_of_iterations_completed / total_num_of_iterations), 3) * 100\n",
        "          progress_percent_bef = round((num_of_iterations_completed_bef / total_num_of_iterations), 3) * 100\n",
        "          if (progress_percent != progress_percent_bef):\n",
        "            #Prints the total percentage of function completed\n",
        "            print(str(progress_percent) +'%')\n",
        "  #Return result determined whether k-fold validation was performed\n",
        "  if cv > 1:\n",
        "    return mae_results, mae_results_sd\n",
        "  else:\n",
        "    return mae_results\n",
        "def zoom_3D_XGB(A_data_arg, mae_upper_limit):\n",
        "  \"\"\"\n",
        "  Allows for the interactive_4Dsurface_XGB to focus on values below a certain mae_upper_limit to enhance\n",
        "  granularity of interactive graph. A_dat_arg is the return value of for_4D_plot_XGB.\n",
        "  \"\"\"\n",
        "  #Copies data, initializes important dictionaries\n",
        "  A_data = A_data_arg.copy()\n",
        "  pruned_data = {}\n",
        "  keys = {}\n",
        "  for param in A_data_arg.keys():\n",
        "    keys[param] = []\n",
        "\n",
        "  #Filters for data rows based on if the 'mae' value is lower then mae_upper_limit\n",
        "  A_data_test = A_data['MAE'] < mae_upper_limit\n",
        "  for k in range(0, len(A_data_test)):\n",
        "    #Clears shuttle_dict for next iteration\n",
        "    shuttle_dict = {} \n",
        "    #Iterates through A_data_test at index k and appends parameters to shuttle_dict\n",
        "    for i in range(0, len(A_data_test[k])):\n",
        "      if A_data_test[k,i]:\n",
        "        for param in keys.keys():\n",
        "          shuttle_dict.setdefault(param, []).append(A_data[param][k,i])\n",
        "    #Appends shuttle_values to pruned_data\n",
        "    for param in shuttle_dict.keys():\n",
        "      pruned_data.setdefault(param, []).append(shuttle_dict[param]) \n",
        "  return pruned_data\n",
        "def zoom_4D_XGB(A_data_arg, mae_upper_limit):\n",
        "  \"\"\"\n",
        "  Allows for the interactive_4Dsurface_XGB to focus on values below a certain mae_upper_limit to enhance\n",
        "  granularity of interactive graph. A_dat_arg is the return value of for_4D_plot_XGB.\n",
        "  \"\"\"\n",
        "  #Copies data, initializes important dictionaries\n",
        "  A_data = A_data_arg.copy()\n",
        "  pruned_data = {}\n",
        "  keys = {'mae':[], 'n_estimators':[],\n",
        "          'learning_rate':[], 'max_depth':[]}\n",
        "  #Filters for data rows based on if the 'mae' value is lower then mae_upper_limit\n",
        "  A_data_test = A_data['mae'] < mae_upper_limit\n",
        "  for k in range(0, len(A_data_test)):\n",
        "    #Clears shuttle_dict for next iteration\n",
        "    shuttle_dict = {} \n",
        "    #Iterates through A_data_test at index k and appends parameters to shuttle_dict\n",
        "    for i in range(0, len(A_data_test[k])):\n",
        "      if A_data_test[k,i]:\n",
        "        for param in keys.keys():\n",
        "          shuttle_dict.setdefault(param, []).append(A_data[param][k,i])\n",
        "    #Appends shuttle_values to pruned_data\n",
        "    for param in shuttle_dict.keys():\n",
        "      pruned_data.setdefault(param, []).append(shuttle_dict[param]) \n",
        "  return pruned_data\n",
        "def test_model_XGB(model_arg, X_arg, y_arg, X_test_arg, preprocessor_arg = None):\n",
        "  \"\"\"\n",
        "  Different from test_model function since it adds an optional preprocessor argument. Same function as \n",
        "  train_model except for the absence of a train_test_split function and no actualy-value output like val_y \n",
        "  in train_model. Used to fit an optimized ML model with the entire predictors data before predicting with\n",
        "  the test predictors dataset.\n",
        "  \"\"\"\n",
        "  if preprocessor_arg == None:\n",
        "    model_arg.fit(X_arg, y_arg)\n",
        "    return model_arg.predict(X_test_arg)\n",
        "  else:\n",
        "    pipeline_model = initialize_Pipeline(preprocessor, model_arg)\n",
        "    pipeline_model.fit(X_arg, y_arg)\n",
        "    return pipeline_model.predict(X_test_arg)\n",
        "    \n",
        "### Abbreviated Functions\n",
        "### ---------------------\n",
        "### For information consult Section II on Taxonomy of Functions\n",
        "def iXG(n_estimators_arg, learning_rate_arg, max_depth_arg):\n",
        "  \"\"\"\n",
        "  Consult initialize_XGB for documentation.\n",
        "  \"\"\"\n",
        "  return initialize_XGB(n_estimators_arg, learning_rate_arg, max_depth_arg)\n",
        "def e4XG(n_estimators_range_arg, max_depth_num_or_list_arg, learning_rate_range_arg,\n",
        "         X_arg, y_arg, preprocessor_arg, NE_increment = 1, cv = 5, percent_complete = False,\n",
        "         learning_rate_divisor = 100):\n",
        "  \"\"\"\n",
        "  Consult experiment4D_with_XGB for documentation.\n",
        "  \"\"\"\n",
        "  return experiment4D_with_XGB(n_estimators_range_arg, max_depth_num_or_list_arg, learning_rate_range_arg,\n",
        "                               X_arg, y_arg, preprocessor_arg, NE_increment = NE_increment, cv = cv, percent_complete = percent_complete,\n",
        "                               learning_rate_divisor = learning_rate_divisor)\n",
        "def ewXG(n_estimators_range_arg, max_depth_or_learning_rate, X_arg, y_arg, preprocessor_arg, NE_increment = 1,\n",
        "         cv = 5, percent_complete = False, test_max_depth = True, learning_rate_divisor = 100,\n",
        "         default_learning_rate = .1, default_max_depth = 5):\n",
        "  \"\"\"\n",
        "  Consult experiment_with_XGB for documentation.\n",
        "  \"\"\"\n",
        "  return experiment_with_XGB(n_estimators_range_arg, max_depth_or_learning_rate, X_arg, y_arg, preprocessor_arg, NE_increment = NE_increment, \n",
        "                             cv = cv, percent_complete = percent_complete, test_max_depth = test_max_depth, learning_rate_divisor = learning_rate_divisor, \n",
        "                             default_learning_rate = default_learning_rate, default_max_depth = default_max_depth)\n",
        "def f3XG(experiment_with_XGB_res, test_max_depth = True):\n",
        "  \"\"\"\n",
        "  Consult for_3D_plot_XGB for documentation.\n",
        "  \"\"\"\n",
        "  return for_3D_plot_XGB(experiment_with_XGB_res, test_max_depth = test_max_depth)\n",
        "def f4XG(experiment_with_XGB_res):\n",
        "  \"\"\"\n",
        "  Consult for_4D_plot_XGB for documentation.\n",
        "  \"\"\"\n",
        "  return for_4D_plot_XGB(experiment_with_XGB_res)\n",
        "def isXG(title, A_data):\n",
        "  \"\"\"\n",
        "  Consult interactive_surface_XGB for documentation.\n",
        "  \"\"\"\n",
        "  return interactive_surface_XGB(title, A_data)\n",
        "def i4XG(title, A_data):\n",
        "  \"\"\"\n",
        "  Consult interactive_4Dsurface_XGB for documentation.\n",
        "  \"\"\"\n",
        "  return interactive_4Dsurface_XGB(title, A_data)\n",
        "def cpXG(title, A_data, B_data, a_alpha = .8, b_alpha = .8, A_label = '', B_label = '',\n",
        "         a_label_sep = 0, b_label_sep = 0, a_xshift = 0, b_xshift = 0):\n",
        "  \"\"\"\n",
        "  Consult comparison_plot_surface_XGB for documentation.\n",
        "  \"\"\"\n",
        "  return comparison_plot_surface_XGB(title, A_data, B_data, a_alpha = a_alpha, b_alpha = b_alpha, A_label = A_label, B_label = B_label,\n",
        "                                     a_label_sep = a_label_sep, b_label_sep = b_label_sep, a_xshift = a_xshift, b_xshift = b_xshift)\n",
        "def ciXG(title, A_data, B_data, a_alpha = .8, b_alpha = .8, A_label = '', B_label = '',\n",
        "         a_label_sep = 0, b_label_sep = 0, a_xshift = 0, b_xshift = 0):\n",
        "  \"\"\"\n",
        "  Consult comparison_interactive_surface_XGB for documentation.\n",
        "  \"\"\"\n",
        "  return comparison_interactive_surface_XGB(title, A_data, B_data, a_alpha = a_alpha, b_alpha = b_alpha, A_label = A_label, B_label = B_label,\n",
        "                                            a_label_sep = a_label_sep, b_label_sep = b_label_sep, a_xshift = a_xshift, b_xshift = b_xshift)\n",
        "def oXG(DataTable, is_4D = False, test_max_depth = True):\n",
        "  \"\"\"\n",
        "  Consult optimize_XGB for documentation.\n",
        "  \"\"\"\n",
        "  return optimize_XGB(DataTable, is_4D = is_4D, test_max_depth = test_max_depth)\n",
        "def z3XG(A_data_arg, mae_upper_limit):\n",
        "  \"\"\"\n",
        "  Consult zoom_3D_XGB for documentation.\n",
        "  \"\"\"\n",
        "  return zoom_3D_XGB(A_data_arg, mae_upper_limit)\n",
        "def z4XG(A_data_arg, mae_upper_limit):\n",
        "  \"\"\"\n",
        "  Consult zoom_4D_XGB for documentation.\n",
        "  \"\"\"\n",
        "  return zoom_4D_XGB(A_data_arg, mae_upper_limit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFOXw5Z_liWq",
        "colab_type": "text"
      },
      "source": [
        "## Other Datasets\n",
        "ML model must be performing regression for the above functions to work. To simplify the process, only functions of `_XGB` species will be used for visualization and optimization because of their cross-compatibilty with almost all `experiment` functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmYGNv71lhBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Stores githib-hosted link in variable (for information about the data, please refer to the README document)\n",
        "\n",
        "train_data_url = '[Enter Train Data URL]'\n",
        "\n",
        "test_data_url = '[Enter Test Data URL]'\n",
        "\n",
        "#train_data is the data used to train data \n",
        "train_data = pd.read_csv(train_data_url)\n",
        "\n",
        "#test_data is the data used to test the model's performance\n",
        "test_data = pd.read_csv(test_data_url)\n",
        "\n",
        "y = #[Target Variable]\n",
        "X_train = #[Predictors]\n",
        "X_test = #[Predictiors]\n",
        "\n",
        "\n",
        "#Perform Any Preprocessing\n",
        "#[                       ]\n",
        "#[                       ]\n",
        "#[                       ]\n",
        "#[                       ]\n",
        "#[                       ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFRI_IDEnsmC",
        "colab_type": "text"
      },
      "source": [
        "### Data Collection\n",
        "Consult the following functions:\n",
        "1. `experiment_with_DT(...)`\n",
        "2. `experiment_with_Forest(...)`\n",
        "3. `experiment_with_PipelineCV_Forest(...)`\n",
        "4. `experiment_with_XGB(...)`\n",
        "5. `experiment4D_with_XGB(...)`\n",
        "\n",
        "Make sure to set the boolean to `percent_complete = True` for functions 2-5 since with large ranges they can take hours to complete."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YmpHii2of4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exp_data = #[Chosen Function Here]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02bdSEW2op62",
        "colab_type": "text"
      },
      "source": [
        "### Visualization\n",
        "Depending on the `experiment` function `species` (explained in [`II Taxonomy of Functions`](https://github.com/rajtum/Machine-Learning-Makeshift-Portfolio/blob/master/Sections/II%20Taxonomy%20of%20Functions.md) but essentially the `_DT`, `_Forest`, and `_XGB` at the end of `experiment_with`), there will be different plotting functions.\n",
        "\n",
        "* Function 1 is a simple line line graph, thus a visualization function was not created to accompany this optimization function.\n",
        "\n",
        "#### For functions 2 - 4:\n",
        "\n",
        "> `for_3D_plot_XGB(experiment_results, test_max_depth = True)`\n",
        "\n",
        "If function 4 tested `learning_rate` instead of `max_depth`:\n",
        "\n",
        "> `for_3D_plot_XGB(experiment_results, test_max_depth = False)`\n",
        "\n",
        "#### For function 5:\n",
        "> `for_4D_plot_XGB(experiment_4Dwith_XGB_results)`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-eDnFNkpXNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#exp_data will be first argument and A_data represents a dictionary of plottable 2D array data\n",
        "A_data = #[Choose for_ function]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZanXqiZKtc50",
        "colab_type": "text"
      },
      "source": [
        "#### Surface Plot\n",
        "For functions 2- 4:\n",
        "\n",
        "> `interactive_surface_XGB(title, A_data)`\n",
        "\n",
        "To zoom into the graph:\n",
        "\n",
        "> `zoom_3D_XGB(A_data_arg, mae_upper_limit)`\n",
        "\n",
        "For function 5:\n",
        "\n",
        "> `interactive_4Dsurface_XGB(title, A_data)`\n",
        "\n",
        "To zoom into the graph:\n",
        "\n",
        "> `zoom_4D_XGB(A_data_arg, mae_upper_limit)`\n",
        "\n",
        "#### Zoom\n",
        "\n",
        "Place the family `zoom` function for the `A_data` argument in the family `interactive` function:\n",
        "\n",
        "> `interactive_4Dsurface_XGB(title, zoom_4D_XGB(A_data_arg, mae_upper_limit)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81GHjEMvtbgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interactive_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oTcepTYBoy27"
      },
      "source": [
        "### Optimization\n",
        "For functions 2 - 4:\n",
        "> `optimize_XGB(exp_data, is_4D = False, test_max_depth = True)`\n",
        "* Note if for function 4 you tested `learning_rate`, set `test_max_depth` to `False`\n",
        "\n",
        "For functions 5:\n",
        "> `optimize_XGB(exp_data, is_4D = True)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4675av7v_Jt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimize_XGB(exp_data, is_4D = , test_max_depth = )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
